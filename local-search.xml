<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>LLM里token的理解</title>
    <link href="/2024/09/07/tokenize-tokenID-embedding-in-llm/"/>
    <url>/2024/09/07/tokenize-tokenID-embedding-in-llm/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202409/202409071518.jpg"></p><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>As fist we should know about Machine Learning. It takes numbers as input and output numbers. And the number can be represent anything in our world.</p><p>在训练一个LLM模型（模型可以理解就是一个方程式，无比庞大的方程式），既然是方程式，那么接收的是数字，而不是文字。所以需要将文字转为数字。也就是我们常说的embdedding。要转成什么样的数字呢，当然不是随便的转。我们需要一种转换方式，将单词转为连续值的向量。</p><p>任何数据类型都能被向量化(embed)，如文字、音频视频等。不同类型的数据的embedding需要不同的模型处理，如之前用过的nomic-embed-text是处理文本的，而音频和视频我没找到，有知道的同学可以分享下。embedding其实就是个映射，一个向量映射一个文字、一个句子、一个段落，甚至一整个文档。embedding一句话或一段文字，通常用于RAG（Retrieval Augmented Generation）。RAG就是查询相关内容并生成内容。</p><p>不过LLM通常都有自己的embedding。优化embedding也是LLM训练的一部分。embedding刚开始理解的时候，可以认为是二维坐标图那种，意思相近的单词会聚在一起（向量查询也是这原理）。然而实际的embdding是高维的。例如GPT-2(117M参数和125M参数)使用的embedding是768维。GPT-3(17B参数)使用的是12288维的embedding。embedding维度的大小与模型的隐藏层状态有关系（the model’s hidden states， 不理解的化需重新看看线性回归模型训练的入门）。</p><p>以下是从tokenize -&gt; tokenID -&gt; embedding 的过程示意，不严谨，但方便理解。</p><h1 id="分词-tokenize"><a href="#分词-tokenize" class="headerlink" title="分词(tokenize)"></a>分词(tokenize)</h1><p>讲了那么多embedding, 那么tokenize和embedding有什么关系呢？ tokenize是embedding的前置步骤。tokenize包括token和tokenID。通过分词器(tokenizer)将一个句子转化为一个个单词序列，也就是token。然后再根据这个单词序列生成对应的tokenID。如何分词，分到什么程度，因训练模型而定。例如上面说的段落、句子也可以看作一个token，还有就是普通文本对空格不敏感，而代码对空格就很敏感。<br>代码示意如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br>input_sentence = <span class="hljs-string">&quot;How are you? I&#x27;m fine, thank you. And you?&quot;</span><br>tokenized_text = re.split(<span class="hljs-string">r&#x27;([,.?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, input_sentence)<br>tokens = [t.strip() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tokenized_text <span class="hljs-keyword">if</span> t.strip()]<br><span class="hljs-built_in">print</span>(tokens)<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs output">[&#x27;How&#x27;, &#x27;are&#x27;, &#x27;you&#x27;, &#x27;?&#x27;, &#x27;I&#x27;, &quot;&#x27;&quot;, &#x27;m&#x27;, &#x27;fine&#x27;, &#x27;,&#x27;, &#x27;thank&#x27;, &#x27;you&#x27;, &#x27;.&#x27;, &#x27;And&#x27;, &#x27;you&#x27;, &#x27;?&#x27;]<br><br></code></pre></td></tr></table></figure><h1 id="tokenID"><a href="#tokenID" class="headerlink" title="tokenID"></a>tokenID</h1><p>tokenID是embedding向量的中间步骤。就是将token映射到一个整型数字上就是tokenID。首先要将训练的数据集转为一个词汇表。这词汇表得去重和排序。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">all_words = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(tokens)))<br>voccab_size = <span class="hljs-built_in">len</span>(all_words)<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Vocabulary size: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(voccab_size))<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;Words: &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(all_words))<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs output">Vocabulary size: 12<br>Words: [&quot;&#x27;&quot;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;?&#x27;, &#x27;And&#x27;, &#x27;How&#x27;, &#x27;I&#x27;, &#x27;are&#x27;, &#x27;fine&#x27;, &#x27;m&#x27;, &#x27;thank&#x27;, &#x27;you&#x27;]<br></code></pre></td></tr></table></figure><p>给每个单词分配一个唯一的ID。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab = &#123;token:integer <span class="hljs-keyword">for</span> integer, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(all_words)&#125;<br><span class="hljs-keyword">for</span> token <span class="hljs-keyword">in</span> vocab.keys():<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">&#x27;&#123;&#125; : &#123;&#125;&#x27;</span>.<span class="hljs-built_in">format</span>(token, vocab[token]))<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs output">&#x27; : 0<br>, : 1<br>. : 2<br>? : 3<br>And : 4<br>How : 5<br>I : 6<br>are : 7<br>fine : 8<br>m : 9<br>thank : 10<br>you : 11<br></code></pre></td></tr></table></figure><p>同样的，知道tokenID也可以反推原本的字符串。</p><p>发现上面的问题没，就是我是根据输入的句子来得出tokenID，如果是继续输入第二句呢？所以其实每个模型训练的时候，都会预先处理数据集，把数据集里所有单词都映射到一个固定的ID中。不过，始终会有遗漏。所以可以设置一些特殊字符表示未知token，如&lt;|unk|&gt;表示未知字符, &lt;|endoftext&gt;表示数据集里面的不同数据子集的间隔（可以认为是分割符）。</p><h1 id="BPE编码-Byte-pair-enocoding"><a href="#BPE编码-Byte-pair-enocoding" class="headerlink" title="BPE编码(Byte pair enocoding)"></a>BPE编码(Byte pair enocoding)</h1><p>上面简单的生成tokenID方便理解其原理。实际上用的是BPE编码生存tokenID。BPE相对复杂点，所以直接使用python的开源框架tiktoken。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">pip install tiktoken<br></code></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> tiktoken<br>tokenizer = tiktoken.get_encoding(<span class="hljs-string">&quot;gpt2&quot;</span>)<br>text = <span class="hljs-string">&quot;How are you? &lt;|endoftext|&gt; I&#x27;m fine, thank you. And you?&quot;</span><br>tokenIDs = tokenizer.encode(text, allowed_special=&#123;<span class="hljs-string">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)<br><span class="hljs-built_in">print</span>(tokenIDs)<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs output">[2437, 389, 345, 30, 220, 50256, 314, 1101, 3734, 11, 5875, 345, 13, 843, 345, 30]<br></code></pre></td></tr></table></figure><p>可以看出&lt;|endoftext|&gt;的tokenID是很大50256，跟其他字符编码数字差很远。说明此工具词汇量大概是50257左右。</p><p>BPE可以识别未知单词，原理大概是拆解不认识的单词，然后再编码。想深入了解可自行查询资料。</p><h1 id="生成embedding"><a href="#生成embedding" class="headerlink" title="生成embedding"></a>生成embedding</h1><p>最后一步就是将tokenID转为embedding向量。相信以下这张图大家都见过很多遍了<br><img src="/img/202409/Neural+networks.jpg"></p><p>embedding是作为input输入。维度多一点，变化就可以更多一些。<br>tokenID与embedding模型作映射，tokenID得出一个embedding向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">vocab_size = <span class="hljs-built_in">len</span>(vocab)<br>output_dim = <span class="hljs-number">3</span><br><span class="hljs-keyword">import</span> torch<br>torch.manual_seed(<span class="hljs-number">123</span>)<br>embedding_layer = torch.nn.Embedding(vocab_size, output_dim)<br><span class="hljs-built_in">print</span>(embedding_layer.weight)<br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs output">Parameter containing:<br>tensor([[ 0.3374, -0.1778, -0.3035],<br>        [-0.5880,  0.3486,  0.6603],<br>        [-0.2196, -0.3792,  0.7671],<br>        [-1.1925,  0.6984, -1.4097],<br>        [ 0.1794,  1.8951,  0.4954],<br>        [ 0.2692, -0.0770, -1.0205],<br>        [-0.1690,  0.9178,  1.5810],<br>        [ 1.3010,  1.2753, -0.2010],<br>        [ 0.4965, -1.5723, -0.4845],<br>        [-2.0929, -0.8199, -0.4210],<br>        [-0.9620,  1.2825,  0.8768],<br>        [ 1.6221, -0.9887, -1.7018],<br>        [-0.7498, -1.1285,  0.4135],<br>        [ 0.2892,  2.2473, -0.8036]], requires_grad=True)<br></code></pre></td></tr></table></figure><p>有了embedding模型后，既可以通过tokenID获取embedding向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python">text = <span class="hljs-string">&quot;How are you? I&#x27;m fine, thank you. And you?&quot;</span><br>tokenized_text = re.split(<span class="hljs-string">r&#x27;([,.?_!&quot;()\&#x27;]|--|\s)&#x27;</span>, text) <span class="hljs-comment"># 分词</span><br>all_words = [t.strip() <span class="hljs-keyword">for</span> t <span class="hljs-keyword">in</span> tokenized_text <span class="hljs-keyword">if</span> t.strip()] <span class="hljs-comment"># 过滤</span><br>sorted_words = <span class="hljs-built_in">sorted</span>(<span class="hljs-built_in">list</span>(<span class="hljs-built_in">set</span>(all_words))) <span class="hljs-comment"># 去重排序</span><br>tokens = &#123;token:integer <span class="hljs-keyword">for</span> integer, token <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(sorted_words)&#125; <span class="hljs-comment"># 编码</span><br>tokenIDs = [tokens[s] <span class="hljs-keyword">for</span> s <span class="hljs-keyword">in</span> all_words] <span class="hljs-comment"># 获取tokenID</span><br><span class="hljs-built_in">print</span>(embedding_layer(torch.tensor(tokenIDs))) <span class="hljs-comment"># 将tokenID转为embedding向量</span><br></code></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs output">tensor([[ 0.2692, -0.0770, -1.0205],<br>        [ 1.3010,  1.2753, -0.2010],<br>        [ 1.6221, -0.9887, -1.7018],<br>        [-1.1925,  0.6984, -1.4097],<br>        [-0.1690,  0.9178,  1.5810],<br>        [ 0.3374, -0.1778, -0.3035],<br>        [-2.0929, -0.8199, -0.4210],<br>        [ 0.4965, -1.5723, -0.4845],<br>        [-0.5880,  0.3486,  0.6603],<br>        [-0.9620,  1.2825,  0.8768],<br>        [ 1.6221, -0.9887, -1.7018],<br>        [-0.2196, -0.3792,  0.7671],<br>        [ 0.1794,  1.8951,  0.4954],<br>        [ 1.6221, -0.9887, -1.7018],<br>        [-1.1925,  0.6984, -1.4097]], grad_fn=&lt;EmbeddingBackward0&gt;)<br></code></pre></td></tr></table></figure><p>这样一看embedding不就是随机的向量，这合理吗？是的，这随机向量就是embedding层的权重(weight)，所以这个embedding层里也要经过训练，发向传播、梯度下降，优化权重等，最终得出个embedding模型。这模型接收字符串返回一个向量矩阵。</p><h1 id="生成文本"><a href="#生成文本" class="headerlink" title="生成文本"></a>生成文本</h1><p>额外篇，不理解可以忽略的话可以忽略此小节。<br>虽然LLM非常复杂，但之前说的，类似是一个大方程式，一个概率方程式。输出的是概率矩阵，模型直接输出的叫logits，类似这样:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs output">tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],<br>         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],<br>         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],<br>         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],<br><br>        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],<br>         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],<br>         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],<br>         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],<br>       grad_fn=&lt;UnsafeViewBackward0&gt;)<br></code></pre></td></tr></table></figure><p>然后通过函数计算出哪个类别（tokenID）概率，如<code>torch.softmax</code>函数计算出每个类别的概率，再用<code>torch.argmax</code>函数找到最大概率对应的类别，也就是tokenID，然后tokenID追加到输入的tokenIDs，继续交给模型生成下一个tokenID，直到达到最大tokens数max_tokens。 最后将tokenIDs转为文本输出给用户。</p><p>看完后，有没感觉更强大了，感觉要手搓一个LLM出来。哈哈哈～ 讲得够通俗易懂了吧？ For more information, please contact me: <a href="mailto:&#x6a;&#x65;&#118;&#111;&#x6e;&#x63;&#x6f;&#100;&#101;&#64;&#103;&#109;&#97;&#105;&#x6c;&#x2e;&#x63;&#111;&#x6d;">&#x6a;&#x65;&#118;&#111;&#x6e;&#x63;&#x6f;&#100;&#101;&#64;&#103;&#109;&#97;&#105;&#x6c;&#x2e;&#x63;&#111;&#x6d;</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>LLM as a Service</title>
    <link href="/2024/07/27/llm-as-a-service/"/>
    <url>/2024/07/27/llm-as-a-service/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202407/202407271640.jpg"></p><h1 id="LLM-as-a-Service"><a href="#LLM-as-a-Service" class="headerlink" title="LLM as a Service"></a>LLM as a Service</h1><p>I initially published my project that utilizes LLM as a service. It’s a dictionary that enables me to ponder what distinguishes this project from those developed in Java. Indeed, the former doesn’t require additional server development like Java projects do; it simply relies on LLM.</p><p>As such, I believe “LLM as a service” will be the next generation of software formats. Furthermore, the recent release of LLaMA 3.1 is significant, given that Mark Zuckerberg mentioned that the 405B model can be used for fine-tuning and distilling smaller models. Notably, a small model can now be considered a service. This means you can train your specific “mode” (service) using these large models.</p><h2 id="Why-“LLM-as-a-Service”-Is-Good-for-People"><a href="#Why-“LLM-as-a-Service”-Is-Good-for-People" class="headerlink" title="Why “LLM as a Service” Is Good for People?"></a>Why “LLM as a Service” Is Good for People?</h2><p>Like my project, <a href="https://github.com/jevoncode/book-buddy">Book-buddy</a>, users only need to download or install the app from an app market. They don’t require additional server installation. As a developer, I also benefit, as I don’t need to provide a service, which saves me time and money spent running a service on a cloud platform.</p><h2 id="How-Can-We-Achieve-This"><a href="#How-Can-We-Achieve-This" class="headerlink" title="How Can We Achieve This?"></a>How Can We Achieve This?</h2><p>ChatGPT has made AI popular recently, so I’m also venturing into this new territory. However, I have some thoughts to share. Firstly, “LLM as a service” requires a suitable <strong>model</strong> and <strong>prompt</strong>. Models typically come from large companies due to the high costs of training them. Prompt engineers are akin to Java developers in the past. Secondly, to achieve user-friendliness, I believe we need to provide a good <strong>UI</strong>. This way, UI developers can still thrive while working on specific aspects.</p><p>That’s all for now. I will continue to pursue the vision of LLM as a service.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>Ollama environment variable setting on Linux</title>
    <link href="/2024/07/23/Ollama-environment-variable-setting-on-Linux/"/>
    <url>/2024/07/23/Ollama-environment-variable-setting-on-Linux/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202407/working.png"><br><strong>Ollama</strong> is a fantastic tool for running large language models. Its setup is very simple, requiring just one command on Linux.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">curl -fsSL https://ollama.com/install.sh | sh<br></code></pre></td></tr></table></figure><p>However, when other services want to use Ollama, they may encounter issues relate to environment variable settings.<br>For example, by default, Ollama listens only on 127.0.0.1, which can cause problems if you need it to listen on all IPs. Fortunately, Ollama provides an environment varibale <code>OLLAMA_HOST=0.0.0.0</code> that allows it to listen on all IPs.</p><p>This document records how to the environment variables I used for setting up Ollama.</p><h1 id="Step-by-Step"><a href="#Step-by-Step" class="headerlink" title="Step-by-Step"></a>Step-by-Step</h1><ol><li><p>After installing Ollama using the command above, you can edit systemd service by running <code>systemctl edit ollama.service</code>. This will open an editor(default to nano, but you can temporarily change it to vim using <code>export EDITOR=vim</code>).</p></li><li><p>For each environment variable, add a line starting with <code>Environment</code> under Section <code>[Service]</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment">### Editing /etc/systemd/system/ollama.service.d/override.conf</span><br><span class="hljs-comment">### Anything between here and the comment below will become the new contents of the file</span><br><br>[Service]<br>Environment=<span class="hljs-string">&quot;OLLAMA_HOST=0.0.0.0&quot;</span><br>Environment=<span class="hljs-string">&quot;OLLAMA_ORIGINS=chrome-extension://*&quot;</span><br><br><span class="hljs-comment">### Lines below this comment will be discarded</span><br><br><span class="hljs-comment">### /etc/systemd/system/ollama.service</span><br><span class="hljs-comment"># [Unit]</span><br><span class="hljs-comment"># Description=Ollama Service</span><br><span class="hljs-comment"># After=network-online.target</span><br><span class="hljs-comment"># </span><br><br></code></pre></td></tr></table></figure><p>Note that variables should be write between ‘### Anthing..’ and ‘### Lines below this comment will be discarded’. If you don’t follow this format, it won’t work (I learned this the hard way!).</p></li><li><p>Save the file using <code>Ctrl+O</code>, and then exit nano using <code>Ctrl+X</code>.</p></li><li><p>Reload <code>systemd</code> and restart Ollama:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">systemctl daemon-reload &amp;&amp; systemctl restart ollama<br></code></pre></td></tr></table></figure></li></ol><h1 id="Verification"><a href="#Verification" class="headerlink" title="Verification"></a>Verification</h1><p>To verify that the setup was successful, use <code>netstat -tunpl</code> to check the IP and port:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs bash">root@aiserver:/home/jevoncode<span class="hljs-comment"># netstat -tunpl</span><br>Active Internet connections (only servers)<br>Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    <br>tcp6       0      0 :::11434                :::*                    LISTEN      9870/ollama  <br></code></pre></td></tr></table></figure><p>If you see output like this, your setup is compalte.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>English Gramar - Nouns</title>
    <link href="/2024/07/14/english-grammar-nouns/"/>
    <url>/2024/07/14/english-grammar-nouns/</url>
    
    <content type="html"><![CDATA[<p>Here are the basic elements of a sentence. You need to know about the different kinds of nouns.</p><h1 id="Nouns"><a href="#Nouns" class="headerlink" title="Nouns"></a>Nouns</h1><h2 id="Concrete-Nouns"><a href="#Concrete-Nouns" class="headerlink" title="Concrete Nouns"></a>Concrete Nouns</h2><p>Concrete nouns refer to people, places, things, etc. that you can see, smell, taste, hear, or touch, essentially using your five senses.</p><h3 id="People"><a href="#People" class="headerlink" title="People"></a>People</h3><ul><li>a man</li><li>a teacher</li><li>Fanny</li><li>Mr. Smith</li></ul><h3 id="Places"><a href="#Places" class="headerlink" title="Places"></a>Places</h3><ul><li>a house</li><li>a school</li><li>London</li><li>a beach</li></ul><h3 id="Things"><a href="#Things" class="headerlink" title="Things"></a>Things</h3><ul><li>a shoe</li><li>a marker</li><li>a dog</li><li>a pɪzza</li></ul><p>These are all concrete nouns.</p><h2 id="Abstract-Nouns"><a href="#Abstract-Nouns" class="headerlink" title="Abstract Nouns"></a>Abstract Nouns</h2><p>Now, let’s move on to abstract nouns. Abstract nouns, unlike concrete nouns, are ideas, concepts, and emotions. You can’t see an idea, smell a concept, taste an emotion, hear it, or touch it. They are nouns and they exist, but you cannot perceive them with your five senses.</p><p>Examples of abstract nouns include love, time, religion, and rules. These words represent ideas and concepts.</p><h2 id="Common-Nouns-Proper-Nouns"><a href="#Common-Nouns-Proper-Nouns" class="headerlink" title="Common Nouns &amp; Proper Nouns"></a>Common Nouns &amp; Proper Nouns</h2><p>Now, let’s see the difference between common nouns and proper nouns. Both common nouns and proper nouns refer to people, places, things, and ideas.</p><h3 id="Examples"><a href="#Examples" class="headerlink" title="Examples:"></a>Examples:</h3><ul><li>People: ‘a woman’ (common noun) vs. ‘Fanny’ (proper noun with a capital ‘F’).</li><li>Places: ‘a city’ (common noun) vs. ‘London’ (proper noun with a capital ‘L’).</li><li>Things: ‘an animal’ (common noun) vs. ‘Snoopy’ (proper noun with a capital ‘S’) for a specific dog.</li><li>Things: ‘a car’ (common noun) vs. ‘Volvo’ (proper noun with a capital ‘V’) for a specific car brand.</li><li>Groups: ‘a team’ (common noun) vs. ‘Manchester United’ (proper noun with a capital ‘M’ and ‘U’) for a specific football team.</li></ul><p>Remember, proper nouns are always capitalɪzed.</p><table><thead><tr><th>Common Nouns</th><th>Proper Nouns</th></tr></thead><tbody><tr><td>a woman</td><td>Fanny</td></tr><tr><td>a city</td><td>London</td></tr><tr><td>a car</td><td>Volvo</td></tr><tr><td>a team</td><td>Manchester United</td></tr></tbody></table><h2 id="Practice-Sentences"><a href="#Practice-Sentences" class="headerlink" title="Practice Sentences"></a>Practice Sentences</h2><p>Now that we know a lot about nouns in English, let’s practice finding the nouns in a sentence.</p><blockquote><p>In my class at Oxford University, I have many friends. My best friend is Jan. I have a lot of love for her. Jan has a cute dog. Its name is Juju.</p></blockquote><p><strong>Common Nouns:</strong> class, friends, friend, love, dog.<br><strong>Proper Nouns:</strong> Oxford University, Jan, Juju.</p><p>As you probably know, I haven’t mentioned ‘I’, ‘her’, and ‘its’. These are also nouns, but they are pronouns and considered a different category in English.</p><h1 id="Singular-and-Plural-Nouns"><a href="#Singular-and-Plural-Nouns" class="headerlink" title="Singular and Plural Nouns"></a>Singular and Plural Nouns</h1><p>When you speak Enlgish, it is very important to know the difference between a singular noun and a plural noun.</p><p>Ok gus, this first you need to know, is that a singular noun means one. So, for exanple, I can say ‘cat’, ‘a cat’ or ‘one cat’, ‘school’, ‘a school’ or ‘one school’. ‘team’, don’t forget, ‘team’ is a collective noun. It’s a group of people, but still, it is a singular noun. We talk about ‘a team’, or ‘one team’.<br>Plural nous are more than one. So, for example, two, three, four, or many. If we take our workds again, ‘a cat’ becomes ‘two cats’, 10 cats, 5 cats. ‘school’ becomes ‘schools’, ‘team’ becomes ‘teams’, Ok, so you just add an ‘s’. ‘lady’ becomes ‘ladies’, But two different rules. As you can see ‘lady’ is consonant + ‘y’. Now when you have consonant + ‘y’, in an English word, the plural wiil be ‘ies’. ‘lady’, ‘lady’. But when you have vowel + ‘y’ like ‘monkeys’, it just becomes ‘monkeys’. You simply add an ‘s’. <code>tomato</code> and <code>piano</code> are two diffrent rules. ‘tomato’ becomes ‘tomatoes’, you add ‘es’. With most words ending in ‘o’, so consonant + ‘o’, you will add ‘es’. But sometimes, you will only add ‘s’. Like ‘piano’, ‘pianos’. There is no particular rule for this. You just need to know the words that only end with an ‘s’.</p><table><thead><tr><th>Singular Nouns</th><th>Plural Nouns</th></tr></thead><tbody><tr><td>cat</td><td>cats</td></tr><tr><td>school</td><td>schools</td></tr><tr><td>team</td><td>teams</td></tr><tr><td>lady</td><td>ladies</td></tr><tr><td>monkey</td><td>monkeys</td></tr><tr><td>tomato</td><td>tomatoes</td></tr><tr><td>piano</td><td>pianos</td></tr></tbody></table><h2 id="Pronunciation-Practice"><a href="#Pronunciation-Practice" class="headerlink" title="Pronunciation Practice"></a>Pronunciation Practice</h2><p>Ok, let’s move on to some pronunciation.<br>So, when it comes to pronunciation, we have three diferent sounds.</p><ul><li>&#x2F;s&#x2F;</li><li>&#x2F;z&#x2F;</li><li>&#x2F;ɪz&#x2F;</li></ul><p>The first sound is &#x2F;s&#x2F;. The second sound is &#x2F;z&#x2F;, and the third one is &#x2F;ɪz&#x2F;. So let’s review some words together and be really careful, what sound do you hear.</p><table><thead><tr><th>Words</th><th>Sounds</th></tr></thead><tbody><tr><td>cats</td><td>&#x2F;s&#x2F;</td></tr><tr><td>schools</td><td>&#x2F;z&#x2F;</td></tr><tr><td>teams</td><td>&#x2F;z&#x2F;</td></tr><tr><td>ladies</td><td>&#x2F;z&#x2F;</td></tr><tr><td>monkeys</td><td>&#x2F;z&#x2F;</td></tr><tr><td>tomatoes</td><td>&#x2F;z&#x2F;</td></tr><tr><td>pianos</td><td>&#x2F;z&#x2F;</td></tr></tbody></table><p>Let’s movon on tho other rules now.<br>Ok. gus, let’s now tak about nouns that end in ‘s’, ‘sh’, ‘x’, ‘ch’ or ‘z’. Now to make the plural form of these nouns, you will add ‘es’. And the sound will be ‘&#x2F;ɪz&#x2F;‘.<br>Let’s review some words together. </p><table><thead><tr><th>Words</th><th>Sounds</th></tr></thead><tbody><tr><td>bus &#x2F;bʌs&#x2F;</td><td>buses &#x2F;bʌsɪz&#x2F;</td></tr><tr><td>bush &#x2F;bʊʃ&#x2F;</td><td>bushes &#x2F;bʊʃɪz&#x2F;</td></tr><tr><td>fox &#x2F;fɒks&#x2F;</td><td>foxes  &#x2F;fɒksɪz&#x2F;</td></tr><tr><td>beach &#x2F;biːtʃ&#x2F;</td><td>beaches &#x2F;biːtʃɪz&#x2F;</td></tr><tr><td>quiz &#x2F;kwɪz&#x2F;</td><td>quizzes &#x2F;kwɪzɪz&#x2F;</td></tr></tbody></table><p>Moving on to nouns that end in ‘f’ or ‘fe’. For example, ‘roof’ becomes ‘roofs’, ‘safe’ becomes ‘safes’. So you simply add an ‘s’. Then we have ‘leaf’ that becomes ‘leaves’. Wait a minute. What happend?  We, ya, sometims in English, a word ending in ‘f’ becomes a word ending on ‘ves’ inp plural. That’s not a rule. But some words end in ‘ves’, you just have to learn them I’m afraid. Another word, ‘wife’ becomes ‘wives’. ‘shelf’ becomes ‘shelves’, again, this ‘ves’ ending.</p><table><thead><tr><th>Words</th><th>Sounds</th></tr></thead><tbody><tr><td>roof  &#x2F;ruːf&#x2F;</td><td>roofs   &#x2F;ruːfs&#x2F;</td></tr><tr><td>safe &#x2F;seɪf&#x2F;</td><td>safes    &#x2F;seɪfs&#x2F;</td></tr><tr><td>leaff &#x2F;liːv&#x2F;</td><td>leaves    &#x2F;liːvz&#x2F;</td></tr><tr><td>wife &#x2F;waɪf&#x2F;</td><td>wives      &#x2F;waɪvs&#x2F;</td></tr><tr><td>shelf   &#x2F;ʃelf&#x2F;</td><td>shelves    &#x2F;ʃelves&#x2F;</td></tr></tbody></table><p>Extra Practics:</p><table><thead><tr><th>Singular</th><th>Plural</th></tr></thead><tbody><tr><td>baby</td><td>babies</td></tr><tr><td>toy</td><td>toys</td></tr><tr><td>wish</td><td>wishes</td></tr><tr><td>taxi</td><td>taxis</td></tr><tr><td>choice</td><td>choices</td></tr><tr><td>wolf</td><td>wolves</td></tr><tr><td>photo</td><td>photos</td></tr></tbody></table><h2 id="Example-Sentences"><a href="#Example-Sentences" class="headerlink" title="Example Sentences"></a>Example Sentences</h2><p>I have some example sentences for you guys. Using singular and plural nouns. I would like you to repeat the sentences after me. And be really careful to use proper pronunciation. </p><blockquote><p>I want a dog.<br>I like dogs.</p></blockquote><blockquote><p>I don’t want a fox.<br>I don’t like foxes.</p></blockquote><blockquote><p>I bought a watch.<br>I have many watches.</p></blockquote><blockquote><p>I have a new stereo.<br>Now, Ihave two stereos.</p></blockquote><blockquote><p>There’s a knife.<br>There are six knives in the kitchen.</p></blockquote>]]></content>
    
    
    
    <tags>
      
      <tag>English Grammar</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Fine-Tuning a Pretrained Model - A Beginner&#39;s Journey</title>
    <link href="/2024/07/13/fine-tune-a-pretrained-model-with-transformers/"/>
    <url>/2024/07/13/fine-tune-a-pretrained-model-with-transformers/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202407/202407131359.jpg"></p><h1 id="My-Environment"><a href="#My-Environment" class="headerlink" title="My Environment"></a>My Environment</h1><ul><li>CUDA 12.2</li><li>Python 3.10.12</li><li>PyTorch 2.3.1+cu121</li><li>transformers 4.42.0</li><li>datasets 2.20.0</li><li>accelerate 0.32.1</li><li>evaluate 0.4.2</li><li>scikit-learn 1.5.1</li></ul><h1 id="Get-Familiar-With-Datasets"><a href="#Get-Familiar-With-Datasets" class="headerlink" title="Get Familiar With Datasets"></a>Get Familiar With Datasets</h1><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><p>load the dataset from Hugging Face and explore it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)<br><span class="hljs-built_in">print</span>(dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">100</span>])<br><span class="hljs-built_in">print</span>(dataset[<span class="hljs-string">&quot;train&quot;</span>].shape)<br><span class="hljs-built_in">print</span>(dataset)<br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span>&#x27;label&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> &#x27;text&#x27;<span class="hljs-punctuation">:</span> &#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\&#x27;s order<span class="hljs-punctuation">,</span> then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\&#x27;s meal. After watching two people who ordered after me be handed their food<span class="hljs-punctuation">,</span> I asked where mine was. The manager started yelling at the cashiers for \\<span class="hljs-string">&quot;serving off their orders\\&quot;</span> when they didn\&#x27;t have their food. But neither cashier was anywhere near those controls<span class="hljs-punctuation">,</span> and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\&#x27;t make sure that I had everything ON MY RECEIPT<span class="hljs-punctuation">,</span> and never even had the decency to apologize that I felt I was getting poor service.\\nI\&#x27;ve eaten at various McDonalds restaurants for over <span class="hljs-number">30</span> years. I\&#x27;ve worked at more than one location. I expect bad days<span class="hljs-punctuation">,</span> bad moods<span class="hljs-punctuation">,</span> and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;<span class="hljs-punctuation">&#125;</span><br>```scss<br>(<span class="hljs-number">650000</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs css">DatasetDict(&#123;<br>    train: <span class="hljs-built_in">Dataset</span>(&#123;<br>        features: [<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],<br>        num_rows: <span class="hljs-number">650000</span><br>    &#125;)<br>    test: <span class="hljs-built_in">Dataset</span>(&#123;<br>        features: [<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],<br>        num_rows: <span class="hljs-number">50000</span><br>    &#125;)<br>&#125;)<br></code></pre></td></tr></table></figure><p>The data structure contains labels and text. The training dataset contains 650,000 samples, while test dataset contains 50,000 samples.</p><h2 id="Tokenize"><a href="#Tokenize" class="headerlink" title="Tokenize"></a>Tokenize</h2><p>We need a tokenizer to convert the text into tokens used to train the model. We use <code>bert-base-uncased</code> as a tokenizer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, BertTokenizerFast<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)<br><br>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>We will no confuse the <code>examples[&quot;text&quot;]</code> when we are familiar with the data structure. The <code>map</code> method seems to batch tokenize data, with default batch size of 1000. <code>batched=True</code> means batching is enabled. So 650,000 data smaples will be divied into 650 batchs (650,000&#x2F;1000 &#x3D; 650). The <code>tokenize_function</code> will be called 650 times.</p><p>There are too much data to train and it will be very slow on my hardware. Thereforre, I will randomly select 1000 samples from both the trainning and test datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br></code></pre></td></tr></table></figure><p>If you are in debug model, you can see the differences bewteen <code>dataset</code> and <code>tokenized_datasets</code>. The <code>tokenized_datasets</code> contains additional fields such as <code>input_ids</code>, <code>token_type_ids</code> and <code>attention_mask</code>. Let’s delve deeper into what these fields are. Don’t be afraid; this is part of familiarizing yourself with the data.</p><p>I wrote a new code snippet to show what these fields are.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><span class="hljs-comment"># Initialize the tokenizer</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)<br><br><span class="hljs-comment"># Define two sequences</span><br>sequence_1 = <span class="hljs-string">&quot;This is the first sentence.&quot;</span><br>sequence_2 = <span class="hljs-string">&quot;This is the second sentence.&quot;</span><br><br><span class="hljs-comment"># Tokenize the sequences</span><br>encoded_input = tokenizer(sequence_1, sequence_2, padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Print the tokenized output</span><br><span class="hljs-built_in">print</span>(encoded_input)<br><br><span class="hljs-comment"># Print the tokenized output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs: &quot;</span>, encoded_input[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Token Type IDs: &quot;</span>, encoded_input[<span class="hljs-string">&#x27;token_type_ids&#x27;</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Attention Mask: &quot;</span>, encoded_input[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Output: </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span>&#x27;input_ids&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">101</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1148</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1248</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> &#x27;token_type_ids&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> &#x27;attention_mask&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br>```css<br>Input IDs<span class="hljs-punctuation">:</span>  <span class="hljs-punctuation">[</span><span class="hljs-number">101</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1148</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1248</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">]</span><br>Token Type IDs<span class="hljs-punctuation">:</span>  <span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><br>Attention Mask<span class="hljs-punctuation">:</span>  <span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><p>As you can see, the two sentences are tokenized into tokens. There are many <code>0</code>s, but I cut them off because they were too long to show.<br>Let’s compare the sentences and the input_ids, and you will see they corespond to each other. <code>101</code> is <code>[CLS]</code> and <code>119</code> is <code>[SEP]</code>. 102 is also<code>[SEP]</code>. The other numers represent the content of the sentences.</p><p>Special tokens are used to mark the beginning of a sentence. However, <code>token_types_ids</code> provide an additional explicit way to distinguish between sentences. While the <code>[CLS]</code> and <code>[SEP]</code> tokens help the model identify boundaries and structure, <code>token_type_ids</code> add another layer of clarity by explicitly indicating which tokens belong to which segment.</p><p>The <code>attenion_mask</code> is used to mask out the padding tokens. As I mentioned earlier,I cut off the padding tokens to show the essential parts.</p><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><p>Ok, enoght explored the data. Let’s train the model!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> evaluate<br><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># Create arguments for training</span><br>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)<br>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):<br>    logits, labels = eval_pred<br>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)<br><br>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    train_dataset=small_train_dataset,<br>    eval_dataset=small_eval_dataset,<br>    compute_metrics=compute_metrics,<br>)<br><br>trainer.train()<br><br><span class="hljs-comment"># Save the trained model</span><br>model_save_path = <span class="hljs-string">&quot;jc-test-fine-tuned&quot;</span><br>trainer.save_model(model_save_path)<br><br><span class="hljs-comment"># Save the tokenizer</span><br>tokenizer2 = BertTokenizerFast.from_pretrained(<span class="hljs-string">&#x27;google-bert/bert-base-cased&#x27;</span>)<br>tokenizer2.save_pretrained(model_save_path)<br></code></pre></td></tr></table></figure><p>It toke me about 3 minutes to train the model. My GPU is a P100.<br>Ouput:</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-number">100</span>%<span class="hljs-string">|██████████| 375/375 [03:12&lt;00:00,  1.95it/s]</span><br>&#123;&#x27;train_runtime&#x27;: <span class="hljs-number">192.3716</span>, &#x27;train_samples_per_second&#x27;: <span class="hljs-number">15.595</span>, &#x27;train_steps_per_second&#x27;: <span class="hljs-number">1.949</span>, &#x27;train_loss&#x27;: <span class="hljs-number">0.98265625</span>, &#x27;epoch&#x27;: <span class="hljs-number">3.0</span>&#125;<br></code></pre></td></tr></table></figure><p>Honestly, I’m not very sure about the result.For example, the <code>train_loss</code> is 0.98265625, Is that good or bad? In my experience with pytorch, the loss in regression or classification demos is usually much lower, almost 0. However, I’m not familiar with the expectations for large language models(LLMs).</p><p>I’m documenting this as a record, ant his is alsl my first time writing in Enlgish.</p><h1 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br>unmasker = pipeline(<span class="hljs-string">&#x27;fill-mask&#x27;</span>, model=<span class="hljs-string">&#x27;jc-test-fine-tuned&#x27;</span>)<br>result = unmasker(<span class="hljs-string">&quot;My expectations for [MASK] are t rarely high.&quot;</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">[&#123;&#x27;score&#x27;: 0.0024227965623140335, &#x27;token&#x27;: 24736, &#x27;token_str&#x27;: &#x27;Rashid&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Rashid model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0018962057074531913, &#x27;token&#x27;: 24880, &#x27;token_str&#x27;: &#x27;Bose&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Bose model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.00178907613735646, &#x27;token&#x27;: 12416, &#x27;token_str&#x27;: &#x27;##iva&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m aiva model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013699078699573874, &#x27;token&#x27;: 18666, &#x27;token_str&#x27;: &#x27;boarded&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a boarded model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013235692167654634, &#x27;token&#x27;: 21664, &#x27;token_str&#x27;: &#x27;##lett&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m alett model.&quot;&#125;]<br><br></code></pre></td></tr></table></figure><p>This demonstrates the inference using the model we just trained. I don’t fully undenstand the output or its relation to the model, but it seems to be working well. This is the beginning of journey into learning machine learning and my english.</p><p>reference: <a href="https://huggingface.co/docs/transformers/en/training">transformers</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>初心</title>
    <link href="/2024/06/11/stay-true/"/>
    <url>/2024/06/11/stay-true/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202406/image.png"></p><blockquote><p>你们必须掌握许多知识，让它们在你们的头脑中形成一个思维框架，在随后的日子里能自动地运用它们。 ——Charlie Munger</p></blockquote><blockquote><p>You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.  ——Charlie Munger</p></blockquote><span id="more"></span><blockquote><p>过去、现在及我所能遇见的未来，科学是打破循环的最好工具。——Jevoncode</p></blockquote><p>上面有两句Charlie Munger的名言，虽然不知道哪句是他的原文。但整体意思肯定也是这位老者所表达的：学习，形成思维模型，生活中用上！</p><p>在看Charlie Munger的演讲之前，我也是为程序员，也对人工智能了解点，然后巧合看了YJango的视频，哟，生物原来一开始为了“学习”环境给予的信息，用了生命做代价，适者生存，“学习”不了环境变化带来的新信息就得死亡。后面生物演进出现了大脑，有了大脑，生物只需要“遗忘”作为代价，即可学习环境给予的新信息，从而适应环境。</p><p>虽然知道模型在大脑的重要性，但不知道哪些思维模型是大家形成共识比较重要的。于是看了Charlie Munger的演讲，推荐了几个模型：数学模型、会计模型、来自硬科学和工程学的思维模型、生物学、生理学的思维模型、心理学模型等等。</p><p>Charlie讲了很多模型，例如在数学上，你不需要成为数学专家或通过某个应试考试，只需在你学会排列组合原理后，你生活中习惯用上就好。最近在看心理学，也是收益颇丰，有机会再说说。</p><p>Anyway，自然演化的大脑、Charlie Munger说的思维模型、AI人工智能常说的大模型。大家形成共识称这些为：模型。所以这之间应该有其共同之处。所以也是我愿意花时间去探索现在很火热的AI，有什么好的学习AI方法也可以推荐下给我。</p><p>最后就是科学，我也不知道准确表述我所理解的科学，但可以有几个例子：</p><ul><li>飞机是科学吧？而发明莱特兄弟并不是像现在程序员那样，测试都不测试，直接就跑起来，出bug再改（生活充满了这样的例子）。莱特兄弟而是做足了实验，学习理论知识，做风洞测试等。</li><li>在平面上的三角形，两边之和大于第三边。</li></ul><p>所以你理解的科学是什么呢？我觉得上述思维模型中，可以加上科学素养的模型。理论验证可行性，风洞测试兜底。<br>纵观人类历史，人类追求的是繁衍。而繁衍不仅仅生孩子。在心理学最新的肯里克模型里，最高级需求是繁衍，可以理解为“造福子孙后代”。而科学和技术的进步会让人类和平地满足自身的心理模型。当科学技术停滞的时候，必带来纷争，然后进入循环。所以科学技术是破局的关键。</p><p>所以讲了那么多，抛开世间纷纷扰扰，上述就是我的初心：科学、思维模型、AI</p><p><img src="/img/202406/science_model_ai.jpg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
