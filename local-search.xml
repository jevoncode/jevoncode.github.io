<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Fine-Tuning a Pretrained Model - A Beginner&#39;s Journey</title>
    <link href="/2024/07/13/fine-tune-a-pretrained-model-with-transformers/"/>
    <url>/2024/07/13/fine-tune-a-pretrained-model-with-transformers/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202407/202407131359.jpg"></p><h1 id="My-Environment"><a href="#My-Environment" class="headerlink" title="My Environment"></a>My Environment</h1><ul><li>CUDA 12.2</li><li>Python 3.10.12</li><li>PyTorch 2.3.1+cu121</li><li>transformers 4.42.0</li><li>datasets 2.20.0</li><li>accelerate 0.32.1</li><li>evaluate 0.4.2</li><li>scikit-learn 1.5.1</li></ul><h1 id="Get-Familiar-With-Datasets"><a href="#Get-Familiar-With-Datasets" class="headerlink" title="Get Familiar With Datasets"></a>Get Familiar With Datasets</h1><h2 id="Data-Structure"><a href="#Data-Structure" class="headerlink" title="Data Structure"></a>Data Structure</h2><p>load the dataset from Hugging Face and explore it.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> datasets <span class="hljs-keyword">import</span> load_dataset<br>dataset = load_dataset(<span class="hljs-string">&quot;yelp_review_full&quot;</span>)<br><span class="hljs-built_in">print</span>(dataset[<span class="hljs-string">&quot;train&quot;</span>][<span class="hljs-number">100</span>])<br><span class="hljs-built_in">print</span>(dataset[<span class="hljs-string">&quot;train&quot;</span>].shape)<br><span class="hljs-built_in">print</span>(dataset)<br></code></pre></td></tr></table></figure><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span>&#x27;label&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> &#x27;text&#x27;<span class="hljs-punctuation">:</span> &#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\&#x27;s order<span class="hljs-punctuation">,</span> then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\&#x27;s meal. After watching two people who ordered after me be handed their food<span class="hljs-punctuation">,</span> I asked where mine was. The manager started yelling at the cashiers for \\<span class="hljs-string">&quot;serving off their orders\\&quot;</span> when they didn\&#x27;t have their food. But neither cashier was anywhere near those controls<span class="hljs-punctuation">,</span> and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\&#x27;t make sure that I had everything ON MY RECEIPT<span class="hljs-punctuation">,</span> and never even had the decency to apologize that I felt I was getting poor service.\\nI\&#x27;ve eaten at various McDonalds restaurants for over <span class="hljs-number">30</span> years. I\&#x27;ve worked at more than one location. I expect bad days<span class="hljs-punctuation">,</span> bad moods<span class="hljs-punctuation">,</span> and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;<span class="hljs-punctuation">&#125;</span><br>```scss<br>(<span class="hljs-number">650000</span><span class="hljs-punctuation">,</span> <span class="hljs-number">2</span>)<br></code></pre></td></tr></table></figure><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs css">DatasetDict(&#123;<br>    train: <span class="hljs-built_in">Dataset</span>(&#123;<br>        features: [<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],<br>        num_rows: <span class="hljs-number">650000</span><br>    &#125;)<br>    test: <span class="hljs-built_in">Dataset</span>(&#123;<br>        features: [<span class="hljs-string">&#x27;label&#x27;</span>, <span class="hljs-string">&#x27;text&#x27;</span>],<br>        num_rows: <span class="hljs-number">50000</span><br>    &#125;)<br>&#125;)<br></code></pre></td></tr></table></figure><p>The data structure contains labels and text. The training dataset contains 650,000 samples, while test dataset contains 50,000 samples.</p><h2 id="Tokenize"><a href="#Tokenize" class="headerlink" title="Tokenize"></a>Tokenize</h2><p>We need a tokenizer to convert the text into tokens used to train the model. We use <code>bert-base-uncased</code> as a tokenizer.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer, BertTokenizerFast<br><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">tokenize_function</span>(<span class="hljs-params">examples</span>):<br>    <span class="hljs-keyword">return</span> tokenizer(examples[<span class="hljs-string">&quot;text&quot;</span>], padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)<br><br>tokenized_datasets = dataset.<span class="hljs-built_in">map</span>(tokenize_function, batched=<span class="hljs-literal">True</span>)<br></code></pre></td></tr></table></figure><p>We will no confuse the <code>examples[&quot;text&quot;]</code> when we are familiar with the data structure. The <code>map</code> method seems to batch tokenize data, with default batch size of 1000. <code>batched=True</code> means batching is enabled. So 650,000 data smaples will be divied into 650 batchs (650,000&#x2F;1000 &#x3D; 650). The <code>tokenize_function</code> will be called 650 times.</p><p>There are too much data to train and it will be very slow on my hardware. Thereforre, I will randomly select 1000 samples from both the trainning and test datasets.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python">small_train_dataset = tokenized_datasets[<span class="hljs-string">&quot;train&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br>small_eval_dataset = tokenized_datasets[<span class="hljs-string">&quot;test&quot;</span>].shuffle(seed=<span class="hljs-number">42</span>).select(<span class="hljs-built_in">range</span>(<span class="hljs-number">1000</span>))<br></code></pre></td></tr></table></figure><p>If you are in debug model, you can see the differences bewteen <code>dataset</code> and <code>tokenized_datasets</code>. The <code>tokenized_datasets</code> contains additional fields such as <code>input_ids</code>, <code>token_type_ids</code> and <code>attention_mask</code>. Let’s delve deeper into what these fields are. Don’t be afraid; this is part of familiarizing yourself with the data.</p><p>I wrote a new code snippet to show what these fields are.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoTokenizer<br><span class="hljs-comment"># Initialize the tokenizer</span><br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>)<br><br><span class="hljs-comment"># Define two sequences</span><br>sequence_1 = <span class="hljs-string">&quot;This is the first sentence.&quot;</span><br>sequence_2 = <span class="hljs-string">&quot;This is the second sentence.&quot;</span><br><br><span class="hljs-comment"># Tokenize the sequences</span><br>encoded_input = tokenizer(sequence_1, sequence_2, padding=<span class="hljs-string">&quot;max_length&quot;</span>, truncation=<span class="hljs-literal">True</span>)<br><br><span class="hljs-comment"># Print the tokenized output</span><br><span class="hljs-built_in">print</span>(encoded_input)<br><br><span class="hljs-comment"># Print the tokenized output</span><br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Input IDs: &quot;</span>, encoded_input[<span class="hljs-string">&#x27;input_ids&#x27;</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Token Type IDs: &quot;</span>, encoded_input[<span class="hljs-string">&#x27;token_type_ids&#x27;</span>])<br><span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;Attention Mask: &quot;</span>, encoded_input[<span class="hljs-string">&#x27;attention_mask&#x27;</span>])<br></code></pre></td></tr></table></figure><p>Output: </p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><code class="hljs json"><span class="hljs-punctuation">&#123;</span>&#x27;input_ids&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">101</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1148</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1248</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> &#x27;token_type_ids&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">,</span> &#x27;attention_mask&#x27;<span class="hljs-punctuation">:</span> <span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><span class="hljs-punctuation">&#125;</span><br>```css<br>Input IDs<span class="hljs-punctuation">:</span>  <span class="hljs-punctuation">[</span><span class="hljs-number">101</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1148</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1188</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1110</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1103</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1248</span><span class="hljs-punctuation">,</span> <span class="hljs-number">5650</span><span class="hljs-punctuation">,</span> <span class="hljs-number">119</span><span class="hljs-punctuation">,</span> <span class="hljs-number">102</span><span class="hljs-punctuation">]</span><br>Token Type IDs<span class="hljs-punctuation">:</span>  <span class="hljs-punctuation">[</span><span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">0</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><br>Attention Mask<span class="hljs-punctuation">:</span>  <span class="hljs-punctuation">[</span><span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">,</span> <span class="hljs-number">1</span><span class="hljs-punctuation">]</span><br></code></pre></td></tr></table></figure><p>As you can see, the two sentences are tokenized into tokens. There are many <code>0</code>s, but I cut them off because they were too long to show.<br>Let’s compare the sentences and the input_ids, and you will see they corespond to each other. <code>101</code> is <code>[CLS]</code> and <code>119</code> is <code>[SEP]</code>. 102 is also<code>[SEP]</code>. The other numers represent the content of the sentences.</p><p>Special tokens are used to mark the beginning of a sentence. However, <code>token_types_ids</code> provide an additional explicit way to distinguish between sentences. While the <code>[CLS]</code> and <code>[SEP]</code> tokens help the model identify boundaries and structure, <code>token_type_ids</code> add another layer of clarity by explicitly indicating which tokens belong to which segment.</p><p>The <code>attenion_mask</code> is used to mask out the padding tokens. As I mentioned earlier,I cut off the padding tokens to show the essential parts.</p><h1 id="Train"><a href="#Train" class="headerlink" title="Train"></a>Train</h1><p>Ok, enoght explored the data. Let’s train the model!</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><code class="hljs python"><br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForSequenceClassification<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> TrainingArguments, Trainer<br><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np<br><span class="hljs-keyword">import</span> evaluate<br><br>model = AutoModelForSequenceClassification.from_pretrained(<span class="hljs-string">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class="hljs-number">5</span>)<br><br><span class="hljs-comment"># Create arguments for training</span><br>training_args = TrainingArguments(output_dir=<span class="hljs-string">&quot;test_trainer&quot;</span>)<br>metric = evaluate.load(<span class="hljs-string">&quot;accuracy&quot;</span>)<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_metrics</span>(<span class="hljs-params">eval_pred</span>):<br>    logits, labels = eval_pred<br>    predictions = np.argmax(logits, axis=-<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">return</span> metric.compute(predictions=predictions, references=labels)<br><br>trainer = Trainer(<br>    model=model,<br>    args=training_args,<br>    train_dataset=small_train_dataset,<br>    eval_dataset=small_eval_dataset,<br>    compute_metrics=compute_metrics,<br>)<br><br>trainer.train()<br><br><span class="hljs-comment"># Save the trained model</span><br>model_save_path = <span class="hljs-string">&quot;jc-test-fine-tuned&quot;</span><br>trainer.save_model(model_save_path)<br><br><span class="hljs-comment"># Save the tokenizer</span><br>tokenizer2 = BertTokenizerFast.from_pretrained(<span class="hljs-string">&#x27;google-bert/bert-base-cased&#x27;</span>)<br>tokenizer2.save_pretrained(model_save_path)<br></code></pre></td></tr></table></figure><p>It toke me about 3 minutes to train the model. My GPU is a P100.<br>Ouput:</p><figure class="highlight 1c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs 1c"><span class="hljs-number">100</span>%<span class="hljs-string">|██████████| 375/375 [03:12&lt;00:00,  1.95it/s]</span><br>&#123;&#x27;train_runtime&#x27;: <span class="hljs-number">192.3716</span>, &#x27;train_samples_per_second&#x27;: <span class="hljs-number">15.595</span>, &#x27;train_steps_per_second&#x27;: <span class="hljs-number">1.949</span>, &#x27;train_loss&#x27;: <span class="hljs-number">0.98265625</span>, &#x27;epoch&#x27;: <span class="hljs-number">3.0</span>&#125;<br></code></pre></td></tr></table></figure><p>Honestly, I’m not very sure about the result.For example, the <code>train_loss</code> is 0.98265625, Is that good or bad? In my experience with pytorch, the loss in regression or classification demos is usually much lower, almost 0. However, I’m not familiar with the expectations for large language models(LLMs).</p><p>I’m documenting this as a record, ant his is alsl my first time writing in Enlgish.</p><h1 id="Inference"><a href="#Inference" class="headerlink" title="Inference"></a>Inference</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> pipeline<br>unmasker = pipeline(<span class="hljs-string">&#x27;fill-mask&#x27;</span>, model=<span class="hljs-string">&#x27;jc-test-fine-tuned&#x27;</span>)<br>result = unmasker(<span class="hljs-string">&quot;My expectations for [MASK] are t rarely high.&quot;</span>)<br><span class="hljs-built_in">print</span>(result)<br></code></pre></td></tr></table></figure><p>Output:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs s">[&#123;&#x27;score&#x27;: 0.0024227965623140335, &#x27;token&#x27;: 24736, &#x27;token_str&#x27;: &#x27;Rashid&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Rashid model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0018962057074531913, &#x27;token&#x27;: 24880, &#x27;token_str&#x27;: &#x27;Bose&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Bose model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.00178907613735646, &#x27;token&#x27;: 12416, &#x27;token_str&#x27;: &#x27;##iva&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m aiva model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013699078699573874, &#x27;token&#x27;: 18666, &#x27;token_str&#x27;: &#x27;boarded&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a boarded model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013235692167654634, &#x27;token&#x27;: 21664, &#x27;token_str&#x27;: &#x27;##lett&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m alett model.&quot;&#125;]<br><br></code></pre></td></tr></table></figure><p>This demonstrates the inference using the model we just trained. I don’t fully undenstand the output or its relation to the model, but it seems to be working well. This is the beginning of journey into learning machine learning and my english.</p><p>reference: <a href="https://huggingface.co/docs/transformers/en/training">transformers</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>初心</title>
    <link href="/2024/06/11/stay-true/"/>
    <url>/2024/06/11/stay-true/</url>
    
    <content type="html"><![CDATA[<p><img src="/img/202406/image.png"></p><blockquote><p>你们必须掌握许多知识，让它们在你们的头脑中形成一个思维框架，在随后的日子里能自动地运用它们。 ——Charlie Munger</p></blockquote><blockquote><p>You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.  ——Charlie Munger</p></blockquote><span id="more"></span><blockquote><p>过去、现在及我所能遇见的未来，科学是打破循环的最好工具。——Jevoncode</p></blockquote><p>上面有两句Charlie Munger的名言，虽然不知道哪句是他的原文。但整体意思肯定也是这位老者所表达的：学习，形成思维模型，生活中用上！</p><p>在看Charlie Munger的演讲之前，我也是为程序员，也对人工智能了解点，然后巧合看了YJango的视频，哟，生物原来一开始为了“学习”环境给予的信息，用了生命做代价，适者生存，“学习”不了环境变化带来的新信息就得死亡。后面生物演进出现了大脑，有了大脑，生物只需要“遗忘”作为代价，即可学习环境给予的新信息，从而适应环境。</p><p>虽然知道模型在大脑的重要性，但不知道哪些思维模型是大家形成共识比较重要的。于是看了Charlie Munger的演讲，推荐了几个模型：数学模型、会计模型、来自硬科学和工程学的思维模型、生物学、生理学的思维模型、心理学模型等等。</p><p>Charlie讲了很多模型，例如在数学上，你不需要成为数学专家或通过某个应试考试，只需在你学会排列组合原理后，你生活中习惯用上就好。最近在看心理学，也是收益颇丰，有机会再说说。</p><p>Anyway，自然演化的大脑、Charlie Munger说的思维模型、AI人工智能常说的大模型。大家形成共识称这些为：模型。所以这之间应该有其共同之处。所以也是我愿意花时间去探索现在很火热的AI，有什么好的学习AI方法也可以推荐下给我。</p><p>最后就是科学，我也不知道准确表述我所理解的科学，但可以有几个例子：</p><ul><li>飞机是科学吧？而发明莱特兄弟并不是像现在程序员那样，测试都不测试，直接就跑起来，出bug再改（生活充满了这样的例子）。莱特兄弟而是做足了实验，学习理论知识，做风洞测试等。</li><li>在平面上的三角形，两边之和大于第三边。</li></ul><p>所以你理解的科学是什么呢？我觉得上述思维模型中，可以加上科学素养的模型。理论验证可行性，风洞测试兜底。<br>纵观人类历史，人类追求的是繁衍。而繁衍不仅仅生孩子。在心理学最新的肯里克模型里，最高级需求是繁衍，可以理解为“造福子孙后代”。而科学和技术的进步会让人类和平地满足自身的心理模型。当科学技术停滞的时候，必带来纷争，然后进入循环。所以科学技术是破局的关键。</p><p>所以讲了那么多，抛开世间纷纷扰扰，上述就是我的初心：科学、思维模型、AI</p><p><img src="/img/202406/science_model_ai.jpg"></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
