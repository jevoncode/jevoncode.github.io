[{"title":"对token的一些理解","uid":"535743d12a8a545e92039514820aa5eb","slug":"tokenize-tokenID-embedding-in-llm","date":"2024-09-07T07:22:12.000Z","updated":"2024-09-07T11:50:00.037Z","comments":true,"path":"api/articles/tokenize-tokenID-embedding-in-llm.json","keywords":null,"cover":"/img/202409/202409071518.jpg","text":" 概述As fist we should know about Machine Learning. It takes numbers as input and ...","permalink":"/post/tokenize-tokenID-embedding-in-llm","photos":[],"count_time":{"symbolsCount":"6.4k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"llm","slug":"llm","count":1,"path":"api/tags/llm.json"},{"name":"token","slug":"token","count":1,"path":"api/tags/token.json"}],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"LLM as a Service","uid":"f682aa67763c04130b86166bc61cd4e7","slug":"llm-as-a-service","date":"2024-07-27T08:34:35.000Z","updated":"2024-09-07T11:47:54.466Z","comments":true,"path":"api/articles/llm-as-a-service.json","keywords":null,"cover":"/img/202407/202407271640.jpg","text":" LLM as a ServiceI initially published my project that utilizes LLM as a service...","permalink":"/post/llm-as-a-service","photos":[],"count_time":{"symbolsCount":"1.6k","symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},{"title":"Ollama environment variable setting on Linux","uid":"6f889a3a77cacd01ccd2898ea1d00f2f","slug":"Ollama-environment-variable-setting-on-Linux","date":"2024-07-23T09:21:20.000Z","updated":"2024-09-07T11:47:51.004Z","comments":true,"path":"api/articles/Ollama-environment-variable-setting-on-Linux.json","keywords":null,"cover":"/img/202407/working.png","text":"Ollama is a fantastic tool for running large language models. Its setup is very ...","permalink":"/post/Ollama-environment-variable-setting-on-Linux","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}]