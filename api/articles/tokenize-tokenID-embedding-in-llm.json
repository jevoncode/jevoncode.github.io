{"title":"对token的一些理解","uid":"535743d12a8a545e92039514820aa5eb","slug":"tokenize-tokenID-embedding-in-llm","date":"2024-09-07T07:22:12.000Z","updated":"2024-09-07T11:50:00.037Z","comments":true,"path":"api/articles/tokenize-tokenID-embedding-in-llm.json","keywords":null,"cover":"/img/202409/202409071518.jpg","content":"<p><img src=\"/img/202409/202409071518.jpg\"></p>\n<h1 id=\"概述\"><a href=\"#概述\" class=\"headerlink\" title=\"概述\"></a>概述</h1><p>As fist we should know about Machine Learning. It takes numbers as input and output numbers. And the number can be represent anything in our world.</p>\n<p>在训练一个LLM模型（模型可以理解就是一个方程式，无比庞大的方程式），既然是方程式，那么接收的是数字，而不是文字。所以需要将文字转为数字。也就是我们常说的embdedding。要转成什么样的数字呢，当然不是随便的转。我们需要一种转换方式，将单词转为连续值的向量。</p>\n<p>任何数据类型都能被向量化(embed)，如文字、音频视频等。不同类型的数据的embedding需要不同的模型处理，如之前用过的nomic-embed-text是处理文本的，而音频和视频我没找到，有知道的同学可以分享下。embedding其实就是个映射，一个向量映射一个文字、一个句子、一个段落，甚至一整个文档。embedding一句话或一段文字，通常用于RAG（Retrieval Augmented Generation）。RAG就是查询相关内容并生成内容。</p>\n<p>不过LLM通常都有自己的embedding。优化embedding也是LLM训练的一部分。embedding刚开始理解的时候，可以认为是二维坐标图那种，意思相近的单词会聚在一起（向量查询也是这原理）。然而实际的embdding是高维的。例如GPT-2(117M参数和125M参数)使用的embedding是768维。GPT-3(17B参数)使用的是12288维的embedding。embedding维度的大小与模型的隐藏层状态有关系（the model’s hidden states， 不理解的化需重新看看线性回归模型训练的入门）。</p>\n<p>以下是从tokenize -&gt; tokenID -&gt; embedding 的过程示意，不严谨，但方便理解。</p>\n<h1 id=\"分词-tokenize\"><a href=\"#分词-tokenize\" class=\"headerlink\" title=\"分词(tokenize)\"></a>分词(tokenize)</h1><p>讲了那么多embedding, 那么tokenize和embedding有什么关系呢？ tokenize是embedding的前置步骤。tokenize包括token和tokenID。通过分词器(tokenizer)将一个句子转化为一个个单词序列，也就是token。然后再根据这个单词序列生成对应的tokenID。如何分词，分到什么程度，因训练模型而定。例如上面说的段落、句子也可以看作一个token，还有就是普通文本对空格不敏感，而代码对空格就很敏感。<br>代码示意如下:</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> re</span><br><span class=\"line\">input_sentence = <span class=\"string\">&quot;How are you? I&#x27;m fine, thank you. And you?&quot;</span></span><br><span class=\"line\">tokenized_text = re.split(<span class=\"string\">r&#x27;([,.?_!&quot;()\\&#x27;]|--|\\s)&#x27;</span>, input_sentence)</span><br><span class=\"line\">tokens = [t.strip() <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> tokenized_text <span class=\"keyword\">if</span> t.strip()]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tokens)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&#x27;How&#x27;, &#x27;are&#x27;, &#x27;you&#x27;, &#x27;?&#x27;, &#x27;I&#x27;, &quot;&#x27;&quot;, &#x27;m&#x27;, &#x27;fine&#x27;, &#x27;,&#x27;, &#x27;thank&#x27;, &#x27;you&#x27;, &#x27;.&#x27;, &#x27;And&#x27;, &#x27;you&#x27;, &#x27;?&#x27;]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"tokenID\"><a href=\"#tokenID\" class=\"headerlink\" title=\"tokenID\"></a>tokenID</h1><p>tokenID是embedding向量的中间步骤。就是将token映射到一个整型数字上就是tokenID。首先要将训练的数据集转为一个词汇表。这词汇表得去重和排序。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">all_words = <span class=\"built_in\">sorted</span>(<span class=\"built_in\">list</span>(<span class=\"built_in\">set</span>(tokens)))</span><br><span class=\"line\">voccab_size = <span class=\"built_in\">len</span>(all_words)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Vocabulary size: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(voccab_size))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&#x27;Words: &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(all_words))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Vocabulary size: 12</span><br><span class=\"line\">Words: [&quot;&#x27;&quot;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;?&#x27;, &#x27;And&#x27;, &#x27;How&#x27;, &#x27;I&#x27;, &#x27;are&#x27;, &#x27;fine&#x27;, &#x27;m&#x27;, &#x27;thank&#x27;, &#x27;you&#x27;]</span><br></pre></td></tr></table></figure>\n<p>给每个单词分配一个唯一的ID。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vocab = &#123;token:integer <span class=\"keyword\">for</span> integer, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(all_words)&#125;</span><br><span class=\"line\"><span class=\"keyword\">for</span> token <span class=\"keyword\">in</span> vocab.keys():</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&#x27;&#123;&#125; : &#123;&#125;&#x27;</span>.<span class=\"built_in\">format</span>(token, vocab[token]))</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x27; : 0</span><br><span class=\"line\">, : 1</span><br><span class=\"line\">. : 2</span><br><span class=\"line\">? : 3</span><br><span class=\"line\">And : 4</span><br><span class=\"line\">How : 5</span><br><span class=\"line\">I : 6</span><br><span class=\"line\">are : 7</span><br><span class=\"line\">fine : 8</span><br><span class=\"line\">m : 9</span><br><span class=\"line\">thank : 10</span><br><span class=\"line\">you : 11</span><br></pre></td></tr></table></figure>\n\n<p>同样的，知道tokenID也可以反推原本的字符串。</p>\n<p>发现上面的问题没，就是我是根据输入的句子来得出tokenID，如果是继续输入第二句呢？所以其实每个模型训练的时候，都会预先处理数据集，把数据集里所有单词都映射到一个固定的ID中。不过，始终会有遗漏。所以可以设置一些特殊字符表示未知token，如&lt;|unk|&gt;表示未知字符, &lt;|endoftext&gt;表示数据集里面的不同数据子集的间隔（可以认为是分割符）。</p>\n<h1 id=\"BPE编码-Byte-pair-enocoding\"><a href=\"#BPE编码-Byte-pair-enocoding\" class=\"headerlink\" title=\"BPE编码(Byte pair enocoding)\"></a>BPE编码(Byte pair enocoding)</h1><p>上面简单的生成tokenID方便理解其原理。实际上用的是BPE编码生存tokenID。BPE相对复杂点，所以直接使用python的开源框架tiktoken。</p>\n<figure class=\"highlight bash\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install tiktoken</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> tiktoken</span><br><span class=\"line\">tokenizer = tiktoken.get_encoding(<span class=\"string\">&quot;gpt2&quot;</span>)</span><br><span class=\"line\">text = <span class=\"string\">&quot;How are you? &lt;|endoftext|&gt; I&#x27;m fine, thank you. And you?&quot;</span></span><br><span class=\"line\">tokenIDs = tokenizer.encode(text, allowed_special=&#123;<span class=\"string\">&quot;&lt;|endoftext|&gt;&quot;</span>&#125;)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(tokenIDs)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[2437, 389, 345, 30, 220, 50256, 314, 1101, 3734, 11, 5875, 345, 13, 843, 345, 30]</span><br></pre></td></tr></table></figure>\n<p>可以看出&lt;|endoftext|&gt;的tokenID是很大50256，跟其他字符编码数字差很远。说明此工具词汇量大概是50257左右。</p>\n<p>BPE可以识别未知单词，原理大概是拆解不认识的单词，然后再编码。想深入了解可自行查询资料。</p>\n<h1 id=\"生成embedding\"><a href=\"#生成embedding\" class=\"headerlink\" title=\"生成embedding\"></a>生成embedding</h1><p>最后一步就是将tokenID转为embedding向量。相信以下这张图大家都见过很多遍了<br><img src=\"/img/202409/Neural+networks.jpg\"></p>\n<p>embedding是作为input输入。维度多一点，变化就可以更多一些。<br>tokenID与embedding模型作映射，tokenID得出一个embedding向量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vocab_size = <span class=\"built_in\">len</span>(vocab)</span><br><span class=\"line\">output_dim = <span class=\"number\">3</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> torch</span><br><span class=\"line\">torch.manual_seed(<span class=\"number\">123</span>)</span><br><span class=\"line\">embedding_layer = torch.nn.Embedding(vocab_size, output_dim)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(embedding_layer.weight)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Parameter containing:</span><br><span class=\"line\">tensor([[ 0.3374, -0.1778, -0.3035],</span><br><span class=\"line\">        [-0.5880,  0.3486,  0.6603],</span><br><span class=\"line\">        [-0.2196, -0.3792,  0.7671],</span><br><span class=\"line\">        [-1.1925,  0.6984, -1.4097],</span><br><span class=\"line\">        [ 0.1794,  1.8951,  0.4954],</span><br><span class=\"line\">        [ 0.2692, -0.0770, -1.0205],</span><br><span class=\"line\">        [-0.1690,  0.9178,  1.5810],</span><br><span class=\"line\">        [ 1.3010,  1.2753, -0.2010],</span><br><span class=\"line\">        [ 0.4965, -1.5723, -0.4845],</span><br><span class=\"line\">        [-2.0929, -0.8199, -0.4210],</span><br><span class=\"line\">        [-0.9620,  1.2825,  0.8768],</span><br><span class=\"line\">        [ 1.6221, -0.9887, -1.7018],</span><br><span class=\"line\">        [-0.7498, -1.1285,  0.4135],</span><br><span class=\"line\">        [ 0.2892,  2.2473, -0.8036]], requires_grad=True)</span><br></pre></td></tr></table></figure>\n<p>有了embedding模型后，既可以通过tokenID获取embedding向量。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">text = <span class=\"string\">&quot;How are you? I&#x27;m fine, thank you. And you?&quot;</span></span><br><span class=\"line\">tokenized_text = re.split(<span class=\"string\">r&#x27;([,.?_!&quot;()\\&#x27;]|--|\\s)&#x27;</span>, text) <span class=\"comment\"># 分词</span></span><br><span class=\"line\">all_words = [t.strip() <span class=\"keyword\">for</span> t <span class=\"keyword\">in</span> tokenized_text <span class=\"keyword\">if</span> t.strip()] <span class=\"comment\"># 过滤</span></span><br><span class=\"line\">sorted_words = <span class=\"built_in\">sorted</span>(<span class=\"built_in\">list</span>(<span class=\"built_in\">set</span>(all_words))) <span class=\"comment\"># 去重排序</span></span><br><span class=\"line\">tokens = &#123;token:integer <span class=\"keyword\">for</span> integer, token <span class=\"keyword\">in</span> <span class=\"built_in\">enumerate</span>(sorted_words)&#125; <span class=\"comment\"># 编码</span></span><br><span class=\"line\">tokenIDs = [tokens[s] <span class=\"keyword\">for</span> s <span class=\"keyword\">in</span> all_words] <span class=\"comment\"># 获取tokenID</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(embedding_layer(torch.tensor(tokenIDs))) <span class=\"comment\"># 将tokenID转为embedding向量</span></span><br></pre></td></tr></table></figure>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[ 0.2692, -0.0770, -1.0205],</span><br><span class=\"line\">        [ 1.3010,  1.2753, -0.2010],</span><br><span class=\"line\">        [ 1.6221, -0.9887, -1.7018],</span><br><span class=\"line\">        [-1.1925,  0.6984, -1.4097],</span><br><span class=\"line\">        [-0.1690,  0.9178,  1.5810],</span><br><span class=\"line\">        [ 0.3374, -0.1778, -0.3035],</span><br><span class=\"line\">        [-2.0929, -0.8199, -0.4210],</span><br><span class=\"line\">        [ 0.4965, -1.5723, -0.4845],</span><br><span class=\"line\">        [-0.5880,  0.3486,  0.6603],</span><br><span class=\"line\">        [-0.9620,  1.2825,  0.8768],</span><br><span class=\"line\">        [ 1.6221, -0.9887, -1.7018],</span><br><span class=\"line\">        [-0.2196, -0.3792,  0.7671],</span><br><span class=\"line\">        [ 0.1794,  1.8951,  0.4954],</span><br><span class=\"line\">        [ 1.6221, -0.9887, -1.7018],</span><br><span class=\"line\">        [-1.1925,  0.6984, -1.4097]], grad_fn=&lt;EmbeddingBackward0&gt;)</span><br></pre></td></tr></table></figure>\n\n<p>这样一看embedding不就是随机的向量，这合理吗？是的，这随机向量就是embedding层的权重(weight)，所以这个embedding层里也要经过训练，发向传播、梯度下降，优化权重等，最终得出个embedding模型。这模型接收字符串返回一个向量矩阵。</p>\n<h1 id=\"生成文本\"><a href=\"#生成文本\" class=\"headerlink\" title=\"生成文本\"></a>生成文本</h1><p>额外篇，不理解可以忽略的话可以忽略此小节。<br>虽然LLM非常复杂，但之前说的，类似是一个大方程式，一个概率方程式。输出的是概率矩阵，模型直接输出的叫logits，类似这样:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],</span><br><span class=\"line\">         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],</span><br><span class=\"line\">         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],</span><br><span class=\"line\">         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],</span><br><span class=\"line\"></span><br><span class=\"line\">        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],</span><br><span class=\"line\">         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],</span><br><span class=\"line\">         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],</span><br><span class=\"line\">         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],</span><br><span class=\"line\">       grad_fn=&lt;UnsafeViewBackward0&gt;)</span><br></pre></td></tr></table></figure>\n<p>然后通过函数计算出哪个类别（tokenID）概率，如<code>torch.softmax</code>函数计算出每个类别的概率，再用<code>torch.argmax</code>函数找到最大概率对应的类别，也就是tokenID，然后tokenID追加到输入的tokenIDs，继续交给模型生成下一个tokenID，直到达到最大tokens数max_tokens。 最后将tokenIDs转为文本输出给用户。</p>\n<p>看完后，有没感觉更强大了，感觉要手搓一个LLM出来。哈哈哈～ 讲得够通俗易懂了吧？ For more information, please contact me: <a href=\"mailto:&#x6a;&#101;&#x76;&#x6f;&#110;&#x63;&#111;&#100;&#101;&#x40;&#x67;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#111;&#109;\">&#x6a;&#101;&#x76;&#x6f;&#110;&#x63;&#111;&#100;&#101;&#x40;&#x67;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#111;&#109;</a></p>\n","feature":true,"text":" 概述As fist we should know about Machine Learning. It takes numbers as input and ...","permalink":"/post/tokenize-tokenID-embedding-in-llm","photos":[],"count_time":{"symbolsCount":"6.4k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"llm","slug":"llm","count":1,"path":"api/tags/llm.json"},{"name":"token","slug":"token","count":1,"path":"api/tags/token.json"}],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E6%A6%82%E8%BF%B0\"><span class=\"toc-text\">概述</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E5%88%86%E8%AF%8D-tokenize\"><span class=\"toc-text\">分词(tokenize)</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#tokenID\"><span class=\"toc-text\">tokenID</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#BPE%E7%BC%96%E7%A0%81-Byte-pair-enocoding\"><span class=\"toc-text\">BPE编码(Byte pair enocoding)</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%94%9F%E6%88%90embedding\"><span class=\"toc-text\">生成embedding</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#%E7%94%9F%E6%88%90%E6%96%87%E6%9C%AC\"><span class=\"toc-text\">生成文本</span></a></li></ol>","author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{},"next_post":{"title":"LLM as a Service","uid":"f682aa67763c04130b86166bc61cd4e7","slug":"llm-as-a-service","date":"2024-07-27T08:34:35.000Z","updated":"2024-09-07T11:47:54.466Z","comments":true,"path":"api/articles/llm-as-a-service.json","keywords":null,"cover":"/img/202407/202407271640.jpg","text":" LLM as a ServiceI initially published my project that utilizes LLM as a service...","permalink":"/post/llm-as-a-service","photos":[],"count_time":{"symbolsCount":"1.6k","symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}