{"title":"Fine-Tuning a Pretrained Model - A Beginner's Journey","uid":"4a4a8cf5d9534709c09c903af99f624c","slug":"fine-tune-a-pretrained-model-with-transformers","date":"2024-07-13T05:47:04.000Z","updated":"2024-09-07T11:47:57.127Z","comments":true,"path":"api/articles/fine-tune-a-pretrained-model-with-transformers.json","keywords":null,"cover":"/img/202407/202407131359.jpg","content":"<p><img src=\"/img/202407/202407131359.jpg\"></p>\n<h1 id=\"My-Environment\"><a href=\"#My-Environment\" class=\"headerlink\" title=\"My Environment\"></a>My Environment</h1><ul>\n<li>CUDA 12.2</li>\n<li>Python 3.10.12</li>\n<li>PyTorch 2.3.1+cu121</li>\n<li>transformers 4.42.0</li>\n<li>datasets 2.20.0</li>\n<li>accelerate 0.32.1</li>\n<li>evaluate 0.4.2</li>\n<li>scikit-learn 1.5.1</li>\n</ul>\n<h1 id=\"Get-Familiar-With-Datasets\"><a href=\"#Get-Familiar-With-Datasets\" class=\"headerlink\" title=\"Get Familiar With Datasets\"></a>Get Familiar With Datasets</h1><h2 id=\"Data-Structure\"><a href=\"#Data-Structure\" class=\"headerlink\" title=\"Data Structure\"></a>Data Structure</h2><p>load the dataset from Hugging Face and explore it.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> datasets <span class=\"keyword\">import</span> load_dataset</span><br><span class=\"line\">dataset = load_dataset(<span class=\"string\">&quot;yelp_review_full&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dataset[<span class=\"string\">&quot;train&quot;</span>][<span class=\"number\">100</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dataset[<span class=\"string\">&quot;train&quot;</span>].shape)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dataset)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"punctuation\">&#123;</span>&#x27;label&#x27;<span class=\"punctuation\">:</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> &#x27;text&#x27;<span class=\"punctuation\">:</span> &#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\&#x27;s order<span class=\"punctuation\">,</span> then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\&#x27;s meal. After watching two people who ordered after me be handed their food<span class=\"punctuation\">,</span> I asked where mine was. The manager started yelling at the cashiers for \\\\<span class=\"string\">&quot;serving off their orders\\\\&quot;</span> when they didn\\&#x27;t have their food. But neither cashier was anywhere near those controls<span class=\"punctuation\">,</span> and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\&#x27;t make sure that I had everything ON MY RECEIPT<span class=\"punctuation\">,</span> and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\&#x27;ve eaten at various McDonalds restaurants for over <span class=\"number\">30</span> years. I\\&#x27;ve worked at more than one location. I expect bad days<span class=\"punctuation\">,</span> bad moods<span class=\"punctuation\">,</span> and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;<span class=\"punctuation\">&#125;</span></span><br><span class=\"line\">```scss</span><br><span class=\"line\">(<span class=\"number\">650000</span><span class=\"punctuation\">,</span> <span class=\"number\">2</span>)</span><br></pre></td></tr></table></figure>\n<figure class=\"highlight css\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">DatasetDict(&#123;</span><br><span class=\"line\">    train: <span class=\"built_in\">Dataset</span>(&#123;</span><br><span class=\"line\">        features: [<span class=\"string\">&#x27;label&#x27;</span>, <span class=\"string\">&#x27;text&#x27;</span>],</span><br><span class=\"line\">        num_rows: <span class=\"number\">650000</span></span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">    test: <span class=\"built_in\">Dataset</span>(&#123;</span><br><span class=\"line\">        features: [<span class=\"string\">&#x27;label&#x27;</span>, <span class=\"string\">&#x27;text&#x27;</span>],</span><br><span class=\"line\">        num_rows: <span class=\"number\">50000</span></span><br><span class=\"line\">    &#125;)</span><br><span class=\"line\">&#125;)</span><br></pre></td></tr></table></figure>\n<p>The data structure contains labels and text. The training dataset contains 650,000 samples, while test dataset contains 50,000 samples.</p>\n<h2 id=\"Tokenize\"><a href=\"#Tokenize\" class=\"headerlink\" title=\"Tokenize\"></a>Tokenize</h2><p>We need a tokenizer to convert the text into tokens used to train the model. We use <code>bert-base-uncased</code> as a tokenizer.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoTokenizer, BertTokenizerFast</span><br><span class=\"line\"></span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"string\">&quot;google-bert/bert-base-cased&quot;</span>)</span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">tokenize_function</span>(<span class=\"params\">examples</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> tokenizer(examples[<span class=\"string\">&quot;text&quot;</span>], padding=<span class=\"string\">&quot;max_length&quot;</span>, truncation=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">tokenized_datasets = dataset.<span class=\"built_in\">map</span>(tokenize_function, batched=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n<p>We will no confuse the <code>examples[&quot;text&quot;]</code> when we are familiar with the data structure. The <code>map</code> method seems to batch tokenize data, with default batch size of 1000. <code>batched=True</code> means batching is enabled. So 650,000 data smaples will be divied into 650 batchs (650,000&#x2F;1000 &#x3D; 650). The <code>tokenize_function</code> will be called 650 times.</p>\n<p>There are too much data to train and it will be very slow on my hardware. Thereforre, I will randomly select 1000 samples from both the trainning and test datasets.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">small_train_dataset = tokenized_datasets[<span class=\"string\">&quot;train&quot;</span>].shuffle(seed=<span class=\"number\">42</span>).select(<span class=\"built_in\">range</span>(<span class=\"number\">1000</span>))</span><br><span class=\"line\">small_eval_dataset = tokenized_datasets[<span class=\"string\">&quot;test&quot;</span>].shuffle(seed=<span class=\"number\">42</span>).select(<span class=\"built_in\">range</span>(<span class=\"number\">1000</span>))</span><br></pre></td></tr></table></figure>\n\n<p>If you are in debug model, you can see the differences bewteen <code>dataset</code> and <code>tokenized_datasets</code>. The <code>tokenized_datasets</code> contains additional fields such as <code>input_ids</code>, <code>token_type_ids</code> and <code>attention_mask</code>. Let’s delve deeper into what these fields are. Don’t be afraid; this is part of familiarizing yourself with the data.</p>\n<p>I wrote a new code snippet to show what these fields are.</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoTokenizer</span><br><span class=\"line\"><span class=\"comment\"># Initialize the tokenizer</span></span><br><span class=\"line\">tokenizer = AutoTokenizer.from_pretrained(<span class=\"string\">&quot;google-bert/bert-base-cased&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Define two sequences</span></span><br><span class=\"line\">sequence_1 = <span class=\"string\">&quot;This is the first sentence.&quot;</span></span><br><span class=\"line\">sequence_2 = <span class=\"string\">&quot;This is the second sentence.&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Tokenize the sequences</span></span><br><span class=\"line\">encoded_input = tokenizer(sequence_1, sequence_2, padding=<span class=\"string\">&quot;max_length&quot;</span>, truncation=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Print the tokenized output</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(encoded_input)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Print the tokenized output</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Input IDs: &quot;</span>, encoded_input[<span class=\"string\">&#x27;input_ids&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Token Type IDs: &quot;</span>, encoded_input[<span class=\"string\">&#x27;token_type_ids&#x27;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Attention Mask: &quot;</span>, encoded_input[<span class=\"string\">&#x27;attention_mask&#x27;</span>])</span><br></pre></td></tr></table></figure>\n<p>Output: </p>\n<figure class=\"highlight json\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"punctuation\">&#123;</span>&#x27;input_ids&#x27;<span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"number\">101</span><span class=\"punctuation\">,</span> <span class=\"number\">1188</span><span class=\"punctuation\">,</span> <span class=\"number\">1110</span><span class=\"punctuation\">,</span> <span class=\"number\">1103</span><span class=\"punctuation\">,</span> <span class=\"number\">1148</span><span class=\"punctuation\">,</span> <span class=\"number\">5650</span><span class=\"punctuation\">,</span> <span class=\"number\">119</span><span class=\"punctuation\">,</span> <span class=\"number\">102</span><span class=\"punctuation\">,</span> <span class=\"number\">1188</span><span class=\"punctuation\">,</span> <span class=\"number\">1110</span><span class=\"punctuation\">,</span> <span class=\"number\">1103</span><span class=\"punctuation\">,</span> <span class=\"number\">1248</span><span class=\"punctuation\">,</span> <span class=\"number\">5650</span><span class=\"punctuation\">,</span> <span class=\"number\">119</span><span class=\"punctuation\">,</span> <span class=\"number\">102</span><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span> &#x27;token_type_ids&#x27;<span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">]</span><span class=\"punctuation\">,</span> &#x27;attention_mask&#x27;<span class=\"punctuation\">:</span> <span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">]</span><span class=\"punctuation\">&#125;</span></span><br><span class=\"line\">```css</span><br><span class=\"line\">Input IDs<span class=\"punctuation\">:</span>  <span class=\"punctuation\">[</span><span class=\"number\">101</span><span class=\"punctuation\">,</span> <span class=\"number\">1188</span><span class=\"punctuation\">,</span> <span class=\"number\">1110</span><span class=\"punctuation\">,</span> <span class=\"number\">1103</span><span class=\"punctuation\">,</span> <span class=\"number\">1148</span><span class=\"punctuation\">,</span> <span class=\"number\">5650</span><span class=\"punctuation\">,</span> <span class=\"number\">119</span><span class=\"punctuation\">,</span> <span class=\"number\">102</span><span class=\"punctuation\">,</span> <span class=\"number\">1188</span><span class=\"punctuation\">,</span> <span class=\"number\">1110</span><span class=\"punctuation\">,</span> <span class=\"number\">1103</span><span class=\"punctuation\">,</span> <span class=\"number\">1248</span><span class=\"punctuation\">,</span> <span class=\"number\">5650</span><span class=\"punctuation\">,</span> <span class=\"number\">119</span><span class=\"punctuation\">,</span> <span class=\"number\">102</span><span class=\"punctuation\">]</span></span><br><span class=\"line\">Token Type IDs<span class=\"punctuation\">:</span>  <span class=\"punctuation\">[</span><span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">0</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">]</span></span><br><span class=\"line\">Attention Mask<span class=\"punctuation\">:</span>  <span class=\"punctuation\">[</span><span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">,</span> <span class=\"number\">1</span><span class=\"punctuation\">]</span></span><br></pre></td></tr></table></figure>\n<p>As you can see, the two sentences are tokenized into tokens. There are many <code>0</code>s, but I cut them off because they were too long to show.<br>Let’s compare the sentences and the input_ids, and you will see they corespond to each other. <code>101</code> is <code>[CLS]</code> and <code>119</code> is <code>[SEP]</code>. 102 is also<code>[SEP]</code>. The other numers represent the content of the sentences.</p>\n<p>Special tokens are used to mark the beginning of a sentence. However, <code>token_types_ids</code> provide an additional explicit way to distinguish between sentences. While the <code>[CLS]</code> and <code>[SEP]</code> tokens help the model identify boundaries and structure, <code>token_type_ids</code> add another layer of clarity by explicitly indicating which tokens belong to which segment.</p>\n<p>The <code>attenion_mask</code> is used to mask out the padding tokens. As I mentioned earlier,I cut off the padding tokens to show the essential parts.</p>\n<h1 id=\"Train\"><a href=\"#Train\" class=\"headerlink\" title=\"Train\"></a>Train</h1><p>Ok, enoght explored the data. Let’s train the model!</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> AutoModelForSequenceClassification</span><br><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> TrainingArguments, Trainer</span><br><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br><span class=\"line\"><span class=\"keyword\">import</span> evaluate</span><br><span class=\"line\"></span><br><span class=\"line\">model = AutoModelForSequenceClassification.from_pretrained(<span class=\"string\">&quot;google-bert/bert-base-cased&quot;</span>, num_labels=<span class=\"number\">5</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Create arguments for training</span></span><br><span class=\"line\">training_args = TrainingArguments(output_dir=<span class=\"string\">&quot;test_trainer&quot;</span>)</span><br><span class=\"line\">metric = evaluate.load(<span class=\"string\">&quot;accuracy&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">compute_metrics</span>(<span class=\"params\">eval_pred</span>):</span><br><span class=\"line\">    logits, labels = eval_pred</span><br><span class=\"line\">    predictions = np.argmax(logits, axis=-<span class=\"number\">1</span>)</span><br><span class=\"line\">    <span class=\"keyword\">return</span> metric.compute(predictions=predictions, references=labels)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer = Trainer(</span><br><span class=\"line\">    model=model,</span><br><span class=\"line\">    args=training_args,</span><br><span class=\"line\">    train_dataset=small_train_dataset,</span><br><span class=\"line\">    eval_dataset=small_eval_dataset,</span><br><span class=\"line\">    compute_metrics=compute_metrics,</span><br><span class=\"line\">)</span><br><span class=\"line\"></span><br><span class=\"line\">trainer.train()</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Save the trained model</span></span><br><span class=\"line\">model_save_path = <span class=\"string\">&quot;jc-test-fine-tuned&quot;</span></span><br><span class=\"line\">trainer.save_model(model_save_path)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># Save the tokenizer</span></span><br><span class=\"line\">tokenizer2 = BertTokenizerFast.from_pretrained(<span class=\"string\">&#x27;google-bert/bert-base-cased&#x27;</span>)</span><br><span class=\"line\">tokenizer2.save_pretrained(model_save_path)</span><br></pre></td></tr></table></figure>\n<p>It toke me about 3 minutes to train the model. My GPU is a P100.<br>Ouput:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">100%|██████████| 375/375 [03:12&lt;00:00,  1.95it/s]</span><br><span class=\"line\">&#123;&#x27;train_runtime&#x27;: 192.3716, &#x27;train_samples_per_second&#x27;: 15.595, &#x27;train_steps_per_second&#x27;: 1.949, &#x27;train_loss&#x27;: 0.98265625, &#x27;epoch&#x27;: 3.0&#125;</span><br></pre></td></tr></table></figure>\n<p>Honestly, I’m not very sure about the result.For example, the <code>train_loss</code> is 0.98265625, Is that good or bad? In my experience with pytorch, the loss in regression or classification demos is usually much lower, almost 0. However, I’m not familiar with the expectations for large language models(LLMs).</p>\n<p>I’m documenting this as a record, ant his is alsl my first time writing in Enlgish.</p>\n<h1 id=\"Inference\"><a href=\"#Inference\" class=\"headerlink\" title=\"Inference\"></a>Inference</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> transformers <span class=\"keyword\">import</span> pipeline</span><br><span class=\"line\">unmasker = pipeline(<span class=\"string\">&#x27;fill-mask&#x27;</span>, model=<span class=\"string\">&#x27;jc-test-fine-tuned&#x27;</span>)</span><br><span class=\"line\">result = unmasker(<span class=\"string\">&quot;My expectations for [MASK] are t rarely high.&quot;</span>)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(result)</span><br></pre></td></tr></table></figure>\n<p>Output:</p>\n<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">[&#123;&#x27;score&#x27;: 0.0024227965623140335, &#x27;token&#x27;: 24736, &#x27;token_str&#x27;: &#x27;Rashid&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Rashid model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0018962057074531913, &#x27;token&#x27;: 24880, &#x27;token_str&#x27;: &#x27;Bose&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Bose model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.00178907613735646, &#x27;token&#x27;: 12416, &#x27;token_str&#x27;: &#x27;##iva&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m aiva model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013699078699573874, &#x27;token&#x27;: 18666, &#x27;token_str&#x27;: &#x27;boarded&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a boarded model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013235692167654634, &#x27;token&#x27;: 21664, &#x27;token_str&#x27;: &#x27;##lett&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m alett model.&quot;&#125;]</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>This demonstrates the inference using the model we just trained. I don’t fully undenstand the output or its relation to the model, but it seems to be working well. This is the beginning of journey into learning machine learning and my english.</p>\n<p>reference: <a href=\"https://huggingface.co/docs/transformers/en/training\">transformers</a></p>\n","text":" My Environment CUDA 12.2 Python 3.10.12 PyTorch 2.3.1+cu121 transformers 4.42.0...","permalink":"/post/fine-tune-a-pretrained-model-with-transformers","photos":[],"count_time":{"symbolsCount":"9.1k","symbolsTime":"8 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#My-Environment\"><span class=\"toc-text\">My Environment</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Get-Familiar-With-Datasets\"><span class=\"toc-text\">Get Familiar With Datasets</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Data-Structure\"><span class=\"toc-text\">Data Structure</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Tokenize\"><span class=\"toc-text\">Tokenize</span></a></li></ol></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Train\"><span class=\"toc-text\">Train</span></a></li><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#Inference\"><span class=\"toc-text\">Inference</span></a></li></ol>","author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"English Gramar - Nouns","uid":"f9325c60c551a808709264c3df1fe780","slug":"english-grammar-nouns","date":"2024-07-14T14:47:41.000Z","updated":"2024-09-07T11:48:01.090Z","comments":true,"path":"api/articles/english-grammar-nouns.json","keywords":null,"cover":null,"text":"Here are the basic elements of a sentence. You need to know about the different ...","permalink":"/post/english-grammar-nouns","photos":[],"count_time":{"symbolsCount":"6k","symbolsTime":"5 mins."},"categories":[],"tags":[{"name":"English Grammar","slug":"English-Grammar","count":1,"path":"api/tags/English-Grammar.json"}],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}},"next_post":{"title":"初心","uid":"f3dd3d6cb2078ca4158241482a4610ef","slug":"stay-true","date":"2024-06-11T12:06:36.000Z","updated":"2024-09-07T11:47:45.753Z","comments":true,"path":"api/articles/stay-true.json","keywords":null,"cover":"/img/202406/image.png","text":" 你们必须掌握许多知识，让它们在你们的头脑中形成一个思维框架，在随后的日子里能自动地运用它们。 ——Charlie Munger You’ve got to h...","permalink":"/post/stay-true","photos":[],"count_time":{"symbolsCount":"1.2k","symbolsTime":"1 mins."},"categories":[],"tags":[],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}}}}