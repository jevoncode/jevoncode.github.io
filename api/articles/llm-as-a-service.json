{"title":"LLM as a Service","uid":"f682aa67763c04130b86166bc61cd4e7","slug":"llm-as-a-service","date":"2024-07-27T08:34:35.000Z","updated":"2024-09-07T11:47:54.466Z","comments":true,"path":"api/articles/llm-as-a-service.json","keywords":null,"cover":"/img/202407/202407271640.jpg","content":"<p><img src=\"/img/202407/202407271640.jpg\"></p>\n<h1 id=\"LLM-as-a-Service\"><a href=\"#LLM-as-a-Service\" class=\"headerlink\" title=\"LLM as a Service\"></a>LLM as a Service</h1><p>I initially published my project that utilizes LLM as a service. It’s a dictionary that enables me to ponder what distinguishes this project from those developed in Java. Indeed, the former doesn’t require additional server development like Java projects do; it simply relies on LLM.</p>\n<p>As such, I believe “LLM as a service” will be the next generation of software formats. Furthermore, the recent release of LLaMA 3.1 is significant, given that Mark Zuckerberg mentioned that the 405B model can be used for fine-tuning and distilling smaller models. Notably, a small model can now be considered a service. This means you can train your specific “mode” (service) using these large models.</p>\n<h2 id=\"Why-“LLM-as-a-Service”-Is-Good-for-People\"><a href=\"#Why-“LLM-as-a-Service”-Is-Good-for-People\" class=\"headerlink\" title=\"Why “LLM as a Service” Is Good for People?\"></a>Why “LLM as a Service” Is Good for People?</h2><p>Like my project, <a href=\"https://github.com/jevoncode/book-buddy\">Book-buddy</a>, users only need to download or install the app from an app market. They don’t require additional server installation. As a developer, I also benefit, as I don’t need to provide a service, which saves me time and money spent running a service on a cloud platform.</p>\n<h2 id=\"How-Can-We-Achieve-This\"><a href=\"#How-Can-We-Achieve-This\" class=\"headerlink\" title=\"How Can We Achieve This?\"></a>How Can We Achieve This?</h2><p>ChatGPT has made AI popular recently, so I’m also venturing into this new territory. However, I have some thoughts to share. Firstly, “LLM as a service” requires a suitable <strong>model</strong> and <strong>prompt</strong>. Models typically come from large companies due to the high costs of training them. Prompt engineers are akin to Java developers in the past. Secondly, to achieve user-friendliness, I believe we need to provide a good <strong>UI</strong>. This way, UI developers can still thrive while working on specific aspects.</p>\n<p>That’s all for now. I will continue to pursue the vision of LLM as a service.</p>\n","feature":true,"text":" LLM as a ServiceI initially published my project that utilizes LLM as a service...","permalink":"/post/llm-as-a-service","photos":[],"count_time":{"symbolsCount":"1.6k","symbolsTime":"1 mins."},"categories":[],"tags":[],"toc":"<ol class=\"toc\"><li class=\"toc-item toc-level-1\"><a class=\"toc-link\" href=\"#LLM-as-a-Service\"><span class=\"toc-text\">LLM as a Service</span></a><ol class=\"toc-child\"><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#Why-%E2%80%9CLLM-as-a-Service%E2%80%9D-Is-Good-for-People\"><span class=\"toc-text\">Why “LLM as a Service” Is Good for People?</span></a></li><li class=\"toc-item toc-level-2\"><a class=\"toc-link\" href=\"#How-Can-We-Achieve-This\"><span class=\"toc-text\">How Can We Achieve This?</span></a></li></ol></li></ol>","author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"mapped":true,"hidden":false,"prev_post":{"title":"对token的一些理解","uid":"535743d12a8a545e92039514820aa5eb","slug":"tokenize-tokenID-embedding-in-llm","date":"2024-09-07T07:22:12.000Z","updated":"2024-09-07T11:50:00.037Z","comments":true,"path":"api/articles/tokenize-tokenID-embedding-in-llm.json","keywords":null,"cover":"/img/202409/202409071518.jpg","text":" 概述As fist we should know about Machine Learning. It takes numbers as input and ...","permalink":"/post/tokenize-tokenID-embedding-in-llm","photos":[],"count_time":{"symbolsCount":"6.4k","symbolsTime":"6 mins."},"categories":[],"tags":[{"name":"llm","slug":"llm","count":1,"path":"api/tags/llm.json"},{"name":"token","slug":"token","count":1,"path":"api/tags/token.json"}],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true},"next_post":{"title":"Ollama environment variable setting on Linux","uid":"6f889a3a77cacd01ccd2898ea1d00f2f","slug":"Ollama-environment-variable-setting-on-Linux","date":"2024-07-23T09:21:20.000Z","updated":"2024-09-07T11:47:51.004Z","comments":true,"path":"api/articles/Ollama-environment-variable-setting-on-Linux.json","keywords":null,"cover":"/img/202407/working.png","text":"Ollama is a fantastic tool for running large language models. Its setup is very ...","permalink":"/post/Ollama-environment-variable-setting-on-Linux","photos":[],"count_time":{"symbolsCount":"2.2k","symbolsTime":"2 mins."},"categories":[],"tags":[],"author":{"name":"Jevoncode","slug":"blog-author","avatar":"img/favicon.jpg","link":"/","description":"You’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.","socials":{"github":"https://github.com/jevoncode","twitter":"","stackoverflow":"","wechat":"","qq":"","weibo":"","zhihu":"","csdn":"","juejin":"","customs":{}}},"feature":true}}