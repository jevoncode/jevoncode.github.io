[{"id":"535743d12a8a545e92039514820aa5eb","title":"对token的一些理解","content":"\n概述As fist we should know about Machine Learning. It takes numbers as input and output numbers. And the number can be represent anything in our world.\n在训练一个LLM模型（模型可以理解就是一个方程式，无比庞大的方程式），既然是方程式，那么接收的是数字，而不是文字。所以需要将文字转为数字。也就是我们常说的embdedding。要转成什么样的数字呢，当然不是随便的转。我们需要一种转换方式，将单词转为连续值的向量。\n任何数据类型都能被向量化(embed)，如文字、音频视频等。不同类型的数据的embedding需要不同的模型处理，如之前用过的nomic-embed-text是处理文本的，而音频和视频我没找到，有知道的同学可以分享下。embedding其实就是个映射，一个向量映射一个文字、一个句子、一个段落，甚至一整个文档。embedding一句话或一段文字，通常用于RAG（Retrieval Augmented Generation）。RAG就是查询相关内容并生成内容。\n不过LLM通常都有自己的embedding。优化embedding也是LLM训练的一部分。embedding刚开始理解的时候，可以认为是二维坐标图那种，意思相近的单词会聚在一起（向量查询也是这原理）。然而实际的embdding是高维的。例如GPT-2(117M参数和125M参数)使用的embedding是768维。GPT-3(17B参数)使用的是12288维的embedding。embedding维度的大小与模型的隐藏层状态有关系（the model’s hidden states， 不理解的化需重新看看线性回归模型训练的入门）。\n以下是从tokenize -&gt; tokenID -&gt; embedding 的过程示意，不严谨，但方便理解。\n分词(tokenize)讲了那么多embedding, 那么tokenize和embedding有什么关系呢？ tokenize是embedding的前置步骤。tokenize包括token和tokenID。通过分词器(tokenizer)将一个句子转化为一个个单词序列，也就是token。然后再根据这个单词序列生成对应的tokenID。如何分词，分到什么程度，因训练模型而定。例如上面说的段落、句子也可以看作一个token，还有就是普通文本对空格不敏感，而代码对空格就很敏感。代码示意如下:\n12345import reinput_sentence = &quot;How are you? I&#x27;m fine, thank you. And you?&quot;tokenized_text = re.split(r&#x27;([,.?_!&quot;()\\&#x27;]|--|\\s)&#x27;, input_sentence)tokens = [t.strip() for t in tokenized_text if t.strip()]print(tokens)\n12[&#x27;How&#x27;, &#x27;are&#x27;, &#x27;you&#x27;, &#x27;?&#x27;, &#x27;I&#x27;, &quot;&#x27;&quot;, &#x27;m&#x27;, &#x27;fine&#x27;, &#x27;,&#x27;, &#x27;thank&#x27;, &#x27;you&#x27;, &#x27;.&#x27;, &#x27;And&#x27;, &#x27;you&#x27;, &#x27;?&#x27;]\n\ntokenIDtokenID是embedding向量的中间步骤。就是将token映射到一个整型数字上就是tokenID。首先要将训练的数据集转为一个词汇表。这词汇表得去重和排序。\n1234all_words = sorted(list(set(tokens)))voccab_size = len(all_words)print(&#x27;Vocabulary size: &#123;&#125;&#x27;.format(voccab_size))print(&#x27;Words: &#123;&#125;&#x27;.format(all_words))\n12Vocabulary size: 12Words: [&quot;&#x27;&quot;, &#x27;,&#x27;, &#x27;.&#x27;, &#x27;?&#x27;, &#x27;And&#x27;, &#x27;How&#x27;, &#x27;I&#x27;, &#x27;are&#x27;, &#x27;fine&#x27;, &#x27;m&#x27;, &#x27;thank&#x27;, &#x27;you&#x27;]\n给每个单词分配一个唯一的ID。\n123vocab = &#123;token:integer for integer, token in enumerate(all_words)&#125;for token in vocab.keys():    print(&#x27;&#123;&#125; : &#123;&#125;&#x27;.format(token, vocab[token]))\n123456789101112&#x27; : 0, : 1. : 2? : 3And : 4How : 5I : 6are : 7fine : 8m : 9thank : 10you : 11\n\n同样的，知道tokenID也可以反推原本的字符串。\n发现上面的问题没，就是我是根据输入的句子来得出tokenID，如果是继续输入第二句呢？所以其实每个模型训练的时候，都会预先处理数据集，把数据集里所有单词都映射到一个固定的ID中。不过，始终会有遗漏。所以可以设置一些特殊字符表示未知token，如&lt;|unk|&gt;表示未知字符, &lt;|endoftext&gt;表示数据集里面的不同数据子集的间隔（可以认为是分割符）。\nBPE编码(Byte pair enocoding)上面简单的生成tokenID方便理解其原理。实际上用的是BPE编码生存tokenID。BPE相对复杂点，所以直接使用python的开源框架tiktoken。\n1pip install tiktoken\n12345import tiktokentokenizer = tiktoken.get_encoding(&quot;gpt2&quot;)text = &quot;How are you? &lt;|endoftext|&gt; I&#x27;m fine, thank you. And you?&quot;tokenIDs = tokenizer.encode(text, allowed_special=&#123;&quot;&lt;|endoftext|&gt;&quot;&#125;)print(tokenIDs)\n1[2437, 389, 345, 30, 220, 50256, 314, 1101, 3734, 11, 5875, 345, 13, 843, 345, 30]\n可以看出&lt;|endoftext|&gt;的tokenID是很大50256，跟其他字符编码数字差很远。说明此工具词汇量大概是50257左右。\nBPE可以识别未知单词，原理大概是拆解不认识的单词，然后再编码。想深入了解可自行查询资料。\n生成embedding最后一步就是将tokenID转为embedding向量。相信以下这张图大家都见过很多遍了\nembedding是作为input输入。维度多一点，变化就可以更多一些。tokenID与embedding模型作映射，tokenID得出一个embedding向量。\n123456vocab_size = len(vocab)output_dim = 3import torchtorch.manual_seed(123)embedding_layer = torch.nn.Embedding(vocab_size, output_dim)print(embedding_layer.weight)\n123456789101112131415Parameter containing:tensor([[ 0.3374, -0.1778, -0.3035],        [-0.5880,  0.3486,  0.6603],        [-0.2196, -0.3792,  0.7671],        [-1.1925,  0.6984, -1.4097],        [ 0.1794,  1.8951,  0.4954],        [ 0.2692, -0.0770, -1.0205],        [-0.1690,  0.9178,  1.5810],        [ 1.3010,  1.2753, -0.2010],        [ 0.4965, -1.5723, -0.4845],        [-2.0929, -0.8199, -0.4210],        [-0.9620,  1.2825,  0.8768],        [ 1.6221, -0.9887, -1.7018],        [-0.7498, -1.1285,  0.4135],        [ 0.2892,  2.2473, -0.8036]], requires_grad=True)\n有了embedding模型后，既可以通过tokenID获取embedding向量。\n1234567text = &quot;How are you? I&#x27;m fine, thank you. And you?&quot;tokenized_text = re.split(r&#x27;([,.?_!&quot;()\\&#x27;]|--|\\s)&#x27;, text) # 分词all_words = [t.strip() for t in tokenized_text if t.strip()] # 过滤sorted_words = sorted(list(set(all_words))) # 去重排序tokens = &#123;token:integer for integer, token in enumerate(sorted_words)&#125; # 编码tokenIDs = [tokens[s] for s in all_words] # 获取tokenIDprint(embedding_layer(torch.tensor(tokenIDs))) # 将tokenID转为embedding向量\n123456789101112131415tensor([[ 0.2692, -0.0770, -1.0205],        [ 1.3010,  1.2753, -0.2010],        [ 1.6221, -0.9887, -1.7018],        [-1.1925,  0.6984, -1.4097],        [-0.1690,  0.9178,  1.5810],        [ 0.3374, -0.1778, -0.3035],        [-2.0929, -0.8199, -0.4210],        [ 0.4965, -1.5723, -0.4845],        [-0.5880,  0.3486,  0.6603],        [-0.9620,  1.2825,  0.8768],        [ 1.6221, -0.9887, -1.7018],        [-0.2196, -0.3792,  0.7671],        [ 0.1794,  1.8951,  0.4954],        [ 1.6221, -0.9887, -1.7018],        [-1.1925,  0.6984, -1.4097]], grad_fn=&lt;EmbeddingBackward0&gt;)\n\n这样一看embedding不就是随机的向量，这合理吗？是的，这随机向量就是embedding层的权重(weight)，所以这个embedding层里也要经过训练，发向传播、梯度下降，优化权重等，最终得出个embedding模型。这模型接收字符串返回一个向量矩阵。\n生成文本额外篇，不理解可以忽略的话可以忽略此小节。虽然LLM非常复杂，但之前说的，类似是一个大方程式，一个概率方程式。输出的是概率矩阵，模型直接输出的叫logits，类似这样:\n12345678910tensor([[[ 0.1381,  0.0077, -0.1963,  ..., -0.0222, -0.1060,  0.1717],         [ 0.3865, -0.8408, -0.6564,  ..., -0.5163,  0.2369, -0.3357],         [ 0.6989, -0.1829, -0.1631,  ...,  0.1472, -0.6504, -0.0056],         [-0.4290,  0.1669, -0.1258,  ...,  1.1579,  0.5303, -0.5549]],        [[ 0.1094, -0.2894, -0.1467,  ..., -0.0557,  0.2911, -0.2824],         [ 0.0882, -0.3552, -0.3527,  ...,  1.2930,  0.0053,  0.1898],         [ 0.6091,  0.4702, -0.4094,  ...,  0.7688,  0.3787, -0.1974],         [-0.0612, -0.0737,  0.4751,  ...,  1.2463, -0.3834,  0.0609]]],       grad_fn=&lt;UnsafeViewBackward0&gt;)\n然后通过函数计算出哪个类别（tokenID）概率，如torch.softmax函数计算出每个类别的概率，再用torch.argmax函数找到最大概率对应的类别，也就是tokenID，然后tokenID追加到输入的tokenIDs，继续交给模型生成下一个tokenID，直到达到最大tokens数max_tokens。 最后将tokenIDs转为文本输出给用户。\n看完后，有没感觉更强大了，感觉要手搓一个LLM出来。哈哈哈～ 讲得够通俗易懂了吧？ For more information, please contact me: &#x6a;&#101;&#x76;&#x6f;&#110;&#x63;&#111;&#100;&#101;&#x40;&#x67;&#x6d;&#x61;&#105;&#x6c;&#x2e;&#x63;&#111;&#109;\n","slug":"tokenize-tokenID-embedding-in-llm","date":"2024-09-07T07:22:12.000Z","categories_index":"","tags_index":"llm,token","author_index":"Jevoncode"},{"id":"f682aa67763c04130b86166bc61cd4e7","title":"LLM as a Service","content":"\nLLM as a ServiceI initially published my project that utilizes LLM as a service. It’s a dictionary that enables me to ponder what distinguishes this project from those developed in Java. Indeed, the former doesn’t require additional server development like Java projects do; it simply relies on LLM.\nAs such, I believe “LLM as a service” will be the next generation of software formats. Furthermore, the recent release of LLaMA 3.1 is significant, given that Mark Zuckerberg mentioned that the 405B model can be used for fine-tuning and distilling smaller models. Notably, a small model can now be considered a service. This means you can train your specific “mode” (service) using these large models.\nWhy “LLM as a Service” Is Good for People?Like my project, Book-buddy, users only need to download or install the app from an app market. They don’t require additional server installation. As a developer, I also benefit, as I don’t need to provide a service, which saves me time and money spent running a service on a cloud platform.\nHow Can We Achieve This?ChatGPT has made AI popular recently, so I’m also venturing into this new territory. However, I have some thoughts to share. Firstly, “LLM as a service” requires a suitable model and prompt. Models typically come from large companies due to the high costs of training them. Prompt engineers are akin to Java developers in the past. Secondly, to achieve user-friendliness, I believe we need to provide a good UI. This way, UI developers can still thrive while working on specific aspects.\nThat’s all for now. I will continue to pursue the vision of LLM as a service.\n","slug":"llm-as-a-service","date":"2024-07-27T08:34:35.000Z","categories_index":"","tags_index":"","author_index":"Jevoncode"},{"id":"6f889a3a77cacd01ccd2898ea1d00f2f","title":"Ollama environment variable setting on Linux","content":"Ollama is a fantastic tool for running large language models. Its setup is very simple, requiring just one command on Linux.\n1curl -fsSL https://ollama.com/install.sh | sh\n\nHowever, when other services want to use Ollama, they may encounter issues relate to environment variable settings.For example, by default, Ollama listens only on 127.0.0.1, which can cause problems if you need it to listen on all IPs. Fortunately, Ollama provides an environment varibale OLLAMA_HOST=0.0.0.0 that allows it to listen on all IPs.\nThis document records how to the environment variables I used for setting up Ollama.\nStep-by-Step\nAfter installing Ollama using the command above, you can edit systemd service by running systemctl edit ollama.service. This will open an editor(default to nano, but you can temporarily change it to vim using export EDITOR=vim).\n\nFor each environment variable, add a line starting with Environment under Section [Service]:\n123456789101112131415### Editing /etc/systemd/system/ollama.service.d/override.conf### Anything between here and the comment below will become the new contents of the file[Service]Environment=&quot;OLLAMA_HOST=0.0.0.0&quot;Environment=&quot;OLLAMA_ORIGINS=chrome-extension://*&quot;### Lines below this comment will be discarded### /etc/systemd/system/ollama.service# [Unit]# Description=Ollama Service# After=network-online.target# \nNote that variables should be write between ‘### Anthing..’ and ‘### Lines below this comment will be discarded’. If you don’t follow this format, it won’t work (I learned this the hard way!).\n\nSave the file using Ctrl+O, and then exit nano using Ctrl+X.\n\nReload systemd and restart Ollama:\n1systemctl daemon-reload &amp;&amp; systemctl restart ollama\n\nVerificationTo verify that the setup was successful, use netstat -tunpl to check the IP and port:\n1234root@aiserver:/home/jevoncode# netstat -tunplActive Internet connections (only servers)Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    tcp6       0      0 :::11434                :::*                    LISTEN      9870/ollama  \nIf you see output like this, your setup is compalte.\n","slug":"Ollama-environment-variable-setting-on-Linux","date":"2024-07-23T09:21:20.000Z","categories_index":"","tags_index":"","author_index":"Jevoncode"},{"id":"f9325c60c551a808709264c3df1fe780","title":"English Gramar - Nouns","content":"Here are the basic elements of a sentence. You need to know about the different kinds of nouns.\nNounsConcrete NounsConcrete nouns refer to people, places, things, etc. that you can see, smell, taste, hear, or touch, essentially using your five senses.\nPeople\na man\na teacher\nFanny\nMr. Smith\n\nPlaces\na house\na school\nLondon\na beach\n\nThings\na shoe\na marker\na dog\na pɪzza\n\nThese are all concrete nouns.\nAbstract NounsNow, let’s move on to abstract nouns. Abstract nouns, unlike concrete nouns, are ideas, concepts, and emotions. You can’t see an idea, smell a concept, taste an emotion, hear it, or touch it. They are nouns and they exist, but you cannot perceive them with your five senses.\nExamples of abstract nouns include love, time, religion, and rules. These words represent ideas and concepts.\nCommon Nouns &amp; Proper NounsNow, let’s see the difference between common nouns and proper nouns. Both common nouns and proper nouns refer to people, places, things, and ideas.\nExamples:\nPeople: ‘a woman’ (common noun) vs. ‘Fanny’ (proper noun with a capital ‘F’).\nPlaces: ‘a city’ (common noun) vs. ‘London’ (proper noun with a capital ‘L’).\nThings: ‘an animal’ (common noun) vs. ‘Snoopy’ (proper noun with a capital ‘S’) for a specific dog.\nThings: ‘a car’ (common noun) vs. ‘Volvo’ (proper noun with a capital ‘V’) for a specific car brand.\nGroups: ‘a team’ (common noun) vs. ‘Manchester United’ (proper noun with a capital ‘M’ and ‘U’) for a specific football team.\n\nRemember, proper nouns are always capitalɪzed.\n\n\n\nCommon Nouns\nProper Nouns\n\n\n\na woman\nFanny\n\n\na city\nLondon\n\n\na car\nVolvo\n\n\na team\nManchester United\n\n\nPractice SentencesNow that we know a lot about nouns in English, let’s practice finding the nouns in a sentence.\n\n\n\n\n\n\n\n\n\nIn my class at Oxford University, I have many friends. My best friend is Jan. I have a lot of love for her. Jan has a cute dog. Its name is Juju.\nCommon Nouns: class, friends, friend, love, dog.Proper Nouns: Oxford University, Jan, Juju.\nAs you probably know, I haven’t mentioned ‘I’, ‘her’, and ‘its’. These are also nouns, but they are pronouns and considered a different category in English.\nSingular and Plural NounsWhen you speak Enlgish, it is very important to know the difference between a singular noun and a plural noun.\nOk gus, this first you need to know, is that a singular noun means one. So, for exanple, I can say ‘cat’, ‘a cat’ or ‘one cat’, ‘school’, ‘a school’ or ‘one school’. ‘team’, don’t forget, ‘team’ is a collective noun. It’s a group of people, but still, it is a singular noun. We talk about ‘a team’, or ‘one team’.Plural nous are more than one. So, for example, two, three, four, or many. If we take our workds again, ‘a cat’ becomes ‘two cats’, 10 cats, 5 cats. ‘school’ becomes ‘schools’, ‘team’ becomes ‘teams’, Ok, so you just add an ‘s’. ‘lady’ becomes ‘ladies’, But two different rules. As you can see ‘lady’ is consonant + ‘y’. Now when you have consonant + ‘y’, in an English word, the plural wiil be ‘ies’. ‘lady’, ‘lady’. But when you have vowel + ‘y’ like ‘monkeys’, it just becomes ‘monkeys’. You simply add an ‘s’. tomato and piano are two diffrent rules. ‘tomato’ becomes ‘tomatoes’, you add ‘es’. With most words ending in ‘o’, so consonant + ‘o’, you will add ‘es’. But sometimes, you will only add ‘s’. Like ‘piano’, ‘pianos’. There is no particular rule for this. You just need to know the words that only end with an ‘s’.\n\n\n\nSingular Nouns\nPlural Nouns\n\n\n\ncat\ncats\n\n\nschool\nschools\n\n\nteam\nteams\n\n\nlady\nladies\n\n\nmonkey\nmonkeys\n\n\ntomato\ntomatoes\n\n\npiano\npianos\n\n\nPronunciation PracticeOk, let’s move on to some pronunciation.So, when it comes to pronunciation, we have three diferent sounds.\n\n&#x2F;s&#x2F;\n&#x2F;z&#x2F;\n&#x2F;ɪz&#x2F;\n\nThe first sound is &#x2F;s&#x2F;. The second sound is &#x2F;z&#x2F;, and the third one is &#x2F;ɪz&#x2F;. So let’s review some words together and be really careful, what sound do you hear.\n\n\n\nWords\nSounds\n\n\n\ncats\n&#x2F;s&#x2F;\n\n\nschools\n&#x2F;z&#x2F;\n\n\nteams\n&#x2F;z&#x2F;\n\n\nladies\n&#x2F;z&#x2F;\n\n\nmonkeys\n&#x2F;z&#x2F;\n\n\ntomatoes\n&#x2F;z&#x2F;\n\n\npianos\n&#x2F;z&#x2F;\n\n\nLet’s movon on tho other rules now.Ok. gus, let’s now tak about nouns that end in ‘s’, ‘sh’, ‘x’, ‘ch’ or ‘z’. Now to make the plural form of these nouns, you will add ‘es’. And the sound will be ‘&#x2F;ɪz&#x2F;‘.Let’s review some words together. \n\n\n\nWords\nSounds\n\n\n\nbus &#x2F;bʌs&#x2F;\nbuses &#x2F;bʌsɪz&#x2F;\n\n\nbush &#x2F;bʊʃ&#x2F;\nbushes &#x2F;bʊʃɪz&#x2F;\n\n\nfox &#x2F;fɒks&#x2F;\nfoxes  &#x2F;fɒksɪz&#x2F;\n\n\nbeach &#x2F;biːtʃ&#x2F;\nbeaches &#x2F;biːtʃɪz&#x2F;\n\n\nquiz &#x2F;kwɪz&#x2F;\nquizzes &#x2F;kwɪzɪz&#x2F;\n\n\nMoving on to nouns that end in ‘f’ or ‘fe’. For example, ‘roof’ becomes ‘roofs’, ‘safe’ becomes ‘safes’. So you simply add an ‘s’. Then we have ‘leaf’ that becomes ‘leaves’. Wait a minute. What happend?  We, ya, sometims in English, a word ending in ‘f’ becomes a word ending on ‘ves’ inp plural. That’s not a rule. But some words end in ‘ves’, you just have to learn them I’m afraid. Another word, ‘wife’ becomes ‘wives’. ‘shelf’ becomes ‘shelves’, again, this ‘ves’ ending.\n\n\n\nWords\nSounds\n\n\n\nroof  &#x2F;ruːf&#x2F;\nroofs   &#x2F;ruːfs&#x2F;\n\n\nsafe &#x2F;seɪf&#x2F;\nsafes    &#x2F;seɪfs&#x2F;\n\n\nleaff &#x2F;liːv&#x2F;\nleaves    &#x2F;liːvz&#x2F;\n\n\nwife &#x2F;waɪf&#x2F;\nwives      &#x2F;waɪvs&#x2F;\n\n\nshelf   &#x2F;ʃelf&#x2F;\nshelves    &#x2F;ʃelves&#x2F;\n\n\nExtra Practics:\n\n\n\nSingular\nPlural\n\n\n\nbaby\nbabies\n\n\ntoy\ntoys\n\n\nwish\nwishes\n\n\ntaxi\ntaxis\n\n\nchoice\nchoices\n\n\nwolf\nwolves\n\n\nphoto\nphotos\n\n\nExample SentencesI have some example sentences for you guys. Using singular and plural nouns. I would like you to repeat the sentences after me. And be really careful to use proper pronunciation. \n\n\n\n\n\n\n\n\n\nI want a dog.I like dogs.\n\n\n\n\n\n\n\n\n\nI don’t want a fox.I don’t like foxes.\n\n\n\n\n\n\n\n\n\nI bought a watch.I have many watches.\n\n\n\n\n\n\n\n\n\nI have a new stereo.Now, Ihave two stereos.\n\n\n\n\n\n\n\n\n\nThere’s a knife.There are six knives in the kitchen.\n","slug":"english-grammar-nouns","date":"2024-07-14T14:47:41.000Z","categories_index":"","tags_index":"English Grammar","author_index":"Jevoncode"},{"id":"4a4a8cf5d9534709c09c903af99f624c","title":"Fine-Tuning a Pretrained Model - A Beginner's Journey","content":"\nMy Environment\nCUDA 12.2\nPython 3.10.12\nPyTorch 2.3.1+cu121\ntransformers 4.42.0\ndatasets 2.20.0\naccelerate 0.32.1\nevaluate 0.4.2\nscikit-learn 1.5.1\n\nGet Familiar With DatasetsData Structureload the dataset from Hugging Face and explore it.\n12345from datasets import load_datasetdataset = load_dataset(&quot;yelp_review_full&quot;)print(dataset[&quot;train&quot;][100])print(dataset[&quot;train&quot;].shape)print(dataset)\n123&#123;&#x27;label&#x27;: 0, &#x27;text&#x27;: &#x27;My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\\\nThe cashier took my friends\\&#x27;s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\\&#x27;s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\\\&quot;serving off their orders\\\\&quot; when they didn\\&#x27;t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\\\nThe manager was rude when giving me my order. She didn\\&#x27;t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\\\nI\\&#x27;ve eaten at various McDonalds restaurants for over 30 years. I\\&#x27;ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!&#x27;&#125;```scss(650000, 2)\n12345678910DatasetDict(&#123;    train: Dataset(&#123;        features: [&#x27;label&#x27;, &#x27;text&#x27;],        num_rows: 650000    &#125;)    test: Dataset(&#123;        features: [&#x27;label&#x27;, &#x27;text&#x27;],        num_rows: 50000    &#125;)&#125;)\nThe data structure contains labels and text. The training dataset contains 650,000 samples, while test dataset contains 50,000 samples.\nTokenizeWe need a tokenizer to convert the text into tokens used to train the model. We use bert-base-uncased as a tokenizer.\n1234567from transformers import AutoTokenizer, BertTokenizerFasttokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)def tokenize_function(examples):    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)tokenized_datasets = dataset.map(tokenize_function, batched=True)\nWe will no confuse the examples[&quot;text&quot;] when we are familiar with the data structure. The map method seems to batch tokenize data, with default batch size of 1000. batched=True means batching is enabled. So 650,000 data smaples will be divied into 650 batchs (650,000&#x2F;1000 &#x3D; 650). The tokenize_function will be called 650 times.\nThere are too much data to train and it will be very slow on my hardware. Thereforre, I will randomly select 1000 samples from both the trainning and test datasets.\n12small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(1000))small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(1000))\n\nIf you are in debug model, you can see the differences bewteen dataset and tokenized_datasets. The tokenized_datasets contains additional fields such as input_ids, token_type_ids and attention_mask. Let’s delve deeper into what these fields are. Don’t be afraid; this is part of familiarizing yourself with the data.\nI wrote a new code snippet to show what these fields are.\n123456789101112131415161718from transformers import AutoTokenizer# Initialize the tokenizertokenizer = AutoTokenizer.from_pretrained(&quot;google-bert/bert-base-cased&quot;)# Define two sequencessequence_1 = &quot;This is the first sentence.&quot;sequence_2 = &quot;This is the second sentence.&quot;# Tokenize the sequencesencoded_input = tokenizer(sequence_1, sequence_2, padding=&quot;max_length&quot;, truncation=True)# Print the tokenized outputprint(encoded_input)# Print the tokenized outputprint(&quot;Input IDs: &quot;, encoded_input[&#x27;input_ids&#x27;])print(&quot;Token Type IDs: &quot;, encoded_input[&#x27;token_type_ids&#x27;])print(&quot;Attention Mask: &quot;, encoded_input[&#x27;attention_mask&#x27;])\nOutput: \n12345&#123;&#x27;input_ids&#x27;: [101, 1188, 1110, 1103, 1148, 5650, 119, 102, 1188, 1110, 1103, 1248, 5650, 119, 102], &#x27;token_type_ids&#x27;: [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1], &#x27;attention_mask&#x27;: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]&#125;```cssInput IDs:  [101, 1188, 1110, 1103, 1148, 5650, 119, 102, 1188, 1110, 1103, 1248, 5650, 119, 102]Token Type IDs:  [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]Attention Mask:  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\nAs you can see, the two sentences are tokenized into tokens. There are many 0s, but I cut them off because they were too long to show.Let’s compare the sentences and the input_ids, and you will see they corespond to each other. 101 is [CLS] and 119 is [SEP]. 102 is also[SEP]. The other numers represent the content of the sentences.\nSpecial tokens are used to mark the beginning of a sentence. However, token_types_ids provide an additional explicit way to distinguish between sentences. While the [CLS] and [SEP] tokens help the model identify boundaries and structure, token_type_ids add another layer of clarity by explicitly indicating which tokens belong to which segment.\nThe attenion_mask is used to mask out the padding tokens. As I mentioned earlier,I cut off the padding tokens to show the essential parts.\nTrainOk, enoght explored the data. Let’s train the model!\n12345678910111213141516171819202122232425262728293031323334from transformers import AutoModelForSequenceClassificationfrom transformers import TrainingArguments, Trainerimport numpy as npimport evaluatemodel = AutoModelForSequenceClassification.from_pretrained(&quot;google-bert/bert-base-cased&quot;, num_labels=5)# Create arguments for trainingtraining_args = TrainingArguments(output_dir=&quot;test_trainer&quot;)metric = evaluate.load(&quot;accuracy&quot;)def compute_metrics(eval_pred):    logits, labels = eval_pred    predictions = np.argmax(logits, axis=-1)    return metric.compute(predictions=predictions, references=labels)trainer = Trainer(    model=model,    args=training_args,    train_dataset=small_train_dataset,    eval_dataset=small_eval_dataset,    compute_metrics=compute_metrics,)trainer.train()# Save the trained modelmodel_save_path = &quot;jc-test-fine-tuned&quot;trainer.save_model(model_save_path)# Save the tokenizertokenizer2 = BertTokenizerFast.from_pretrained(&#x27;google-bert/bert-base-cased&#x27;)tokenizer2.save_pretrained(model_save_path)\nIt toke me about 3 minutes to train the model. My GPU is a P100.Ouput:\n12100%|██████████| 375/375 [03:12&lt;00:00,  1.95it/s]&#123;&#x27;train_runtime&#x27;: 192.3716, &#x27;train_samples_per_second&#x27;: 15.595, &#x27;train_steps_per_second&#x27;: 1.949, &#x27;train_loss&#x27;: 0.98265625, &#x27;epoch&#x27;: 3.0&#125;\nHonestly, I’m not very sure about the result.For example, the train_loss is 0.98265625, Is that good or bad? In my experience with pytorch, the loss in regression or classification demos is usually much lower, almost 0. However, I’m not familiar with the expectations for large language models(LLMs).\nI’m documenting this as a record, ant his is alsl my first time writing in Enlgish.\nInference1234from transformers import pipelineunmasker = pipeline(&#x27;fill-mask&#x27;, model=&#x27;jc-test-fine-tuned&#x27;)result = unmasker(&quot;My expectations for [MASK] are t rarely high.&quot;)print(result)\nOutput:\n12[&#123;&#x27;score&#x27;: 0.0024227965623140335, &#x27;token&#x27;: 24736, &#x27;token_str&#x27;: &#x27;Rashid&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Rashid model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0018962057074531913, &#x27;token&#x27;: 24880, &#x27;token_str&#x27;: &#x27;Bose&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a Bose model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.00178907613735646, &#x27;token&#x27;: 12416, &#x27;token_str&#x27;: &#x27;##iva&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m aiva model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013699078699573874, &#x27;token&#x27;: 18666, &#x27;token_str&#x27;: &#x27;boarded&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m a boarded model.&quot;&#125;, &#123;&#x27;score&#x27;: 0.0013235692167654634, &#x27;token&#x27;: 21664, &#x27;token_str&#x27;: &#x27;##lett&#x27;, &#x27;sequence&#x27;: &quot;Hello I&#x27;m alett model.&quot;&#125;]\nThis demonstrates the inference using the model we just trained. I don’t fully undenstand the output or its relation to the model, but it seems to be working well. This is the beginning of journey into learning machine learning and my english.\nreference: transformers\n","slug":"fine-tune-a-pretrained-model-with-transformers","date":"2024-07-13T05:47:04.000Z","categories_index":"","tags_index":"","author_index":"Jevoncode"},{"id":"f3dd3d6cb2078ca4158241482a4610ef","title":"初心","content":"\n你们必须掌握许多知识，让它们在你们的头脑中形成一个思维框架，在随后的日子里能自动地运用它们。 ——Charlie Munger\n\nYou’ve got to have models in your head and you’ve got to array you experience – both vicarious and direct – onto this latticework of mental models.  ——Charlie Munger\n\n\n过去、现在及我所能遇见的未来，科学是打破循环的最好工具。——Jevoncode\n\n\n上面有两句Charlie Munger的名言，虽然不知道哪句是他的原文。但整体意思肯定也是这位老者所表达的：学习，形成思维模型，生活中用上！\n在看Charlie Munger的演讲之前，我也是为程序员，也对人工智能了解点，然后巧合看了YJango的视频，哟，生物原来一开始为了“学习”环境给予的信息，用了生命做代价，适者生存，“学习”不了环境变化带来的新信息就得死亡。后面生物演进出现了大脑，有了大脑，生物只需要“遗忘”作为代价，即可学习环境给予的新信息，从而适应环境。\n虽然知道模型在大脑的重要性，但不知道哪些思维模型是大家形成共识比较重要的。于是看了Charlie Munger的演讲，推荐了几个模型：数学模型、会计模型、来自硬科学和工程学的思维模型、生物学、生理学的思维模型、心理学模型等等。\nCharlie讲了很多模型，例如在数学上，你不需要成为数学专家或通过某个应试考试，只需在你学会排列组合原理后，你生活中习惯用上就好。最近在看心理学，也是收益颇丰，有机会再说说。\nAnyway，自然演化的大脑、Charlie Munger说的思维模型、AI人工智能常说的大模型。大家形成共识称这些为：模型。所以这之间应该有其共同之处。所以也是我愿意花时间去探索现在很火热的AI，有什么好的学习AI方法也可以推荐下给我。\n最后就是科学，我也不知道准确表述我所理解的科学，但可以有几个例子：\n\n飞机是科学吧？而发明莱特兄弟并不是像现在程序员那样，测试都不测试，直接就跑起来，出bug再改（生活充满了这样的例子）。莱特兄弟而是做足了实验，学习理论知识，做风洞测试等。\n在平面上的三角形，两边之和大于第三边。\n\n所以你理解的科学是什么呢？我觉得上述思维模型中，可以加上科学素养的模型。理论验证可行性，风洞测试兜底。纵观人类历史，人类追求的是繁衍。而繁衍不仅仅生孩子。在心理学最新的肯里克模型里，最高级需求是繁衍，可以理解为“造福子孙后代”。而科学和技术的进步会让人类和平地满足自身的心理模型。当科学技术停滞的时候，必带来纷争，然后进入循环。所以科学技术是破局的关键。\n所以讲了那么多，抛开世间纷纷扰扰，上述就是我的初心：科学、思维模型、AI\n\n","slug":"stay-true","date":"2024-06-11T12:06:36.000Z","categories_index":"","tags_index":"","author_index":"Jevoncode"}]